# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) XGBoost

## Student Requirements

Before this lesson, you should be able to...

- Train and evaluate a random forest with scikit-learn.
- Explain what ensembling is and how bagging works.

## Learning Objectives

After this lesson, you should be able to...

- Train and evaluate an XGBoost model in Python.
- Explain how boosting works at a high level.
- Use `GridSearchCV` for hyperparameter tuning.

## Lesson Modules

[XGBoost](./modules/xgboost.ipynb)

## Assignments to Complete Before Next Session

Get some datasets, e.g. from Kaggle, and try XGBoost on them. Use our discussion channel on Slack to report on what you find.

## Additional Resources

- [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
- [Using XGBoost in Python](https://www.datacamp.com/community/tutorials/xgboost-in-python)
- Course: [Extreme Gradient Boosting with XGBoost](https://www.datacamp.com/courses/extreme-gradient-boosting-with-xgboost?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485028&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=295213501352&utm_targetid=aud-522010995285:dsa-498578053364&utm_loc_interest_ms=&utm_loc_physical_ms=9021462&gclid=Cj0KCQjw5J_mBRDVARIsAGqGLZA3C7JNwMH5LAafMy3Dvk8CINfXKjf2k5k6tV4s8wcujdm15ba4If0aAp-1EALw_wcB)