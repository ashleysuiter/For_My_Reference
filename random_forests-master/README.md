# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Random Forests

## Student Requirements

Before this lesson, you should be able to...

- Define the concepts of cross-validation and overfitting.
- Build and evaluate classification models in sklearn.
- Build decision trees in sklearn.

## Learning Objectives

After this lesson, you should be able to...

- Define "ensembling" and "bagging."
- Explain how random forests differ from basic bagged decision trees.
- Build random forests in sklearn.
- Use grid search to tune random forest hyperparameters.

## Follow-up Steps

Try random forest regression and classification on some datasets, e.g. from Kaggle. Use our discussion channel on Slack to report on what you find.

## Lesson Module

[Ensembles and Random Forests](modules/ensembles_random_forests.ipynb)

## Additional Resources

See [the decision trees lesson](https://git.generalassemb.ly/gandenberger-part-time-data-science/decision_trees/blob/master/README.md).
