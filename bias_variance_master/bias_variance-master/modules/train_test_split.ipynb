{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train/Test Splits\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kevin Markham (DC)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df = pd.read_csv('../assets/data/ames_train.csv')\n",
    "X = ames_df.drop('SalePrice', axis='columns')\n",
    "y = ames_df.loc[:, 'SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Simple Train/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this course we have used *simple train/test splits*:\n",
    "\n",
    "1. Split the data set into two pieces: a **training set** and a **testing set**.\n",
    "2. Fit the model on the **training set**.\n",
    "3. Evaluate the model on the **testing set**.\n",
    "\n",
    "A common rule of thumb that sklearn follows by default is to set aside 25% of your data set for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the `train_test_split` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train/test split on the Ames housing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape before splitting  (1460, 80)\n",
      "X shape after splitting  (1095, 80) (365, 80)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that X_train and X_test comes from splitting X by row\n",
    "print('X shape before splitting ', X.shape)\n",
    "print('X shape after splitting ', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape before splitting  (1460,)\n",
      "y shape after splitting  (1095,) (365,)\n"
     ]
    }
   ],
   "source": [
    "# Confirm that y_train and y_test comes from splitting y by row\n",
    "print('y shape before splitting ', y.shape)\n",
    "print('y shape after splitting ', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_test_split](../assets/images/train_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the `random_state` Parameter\n",
    "\n",
    "The `random_state` is a pseudo-random number that allows us to reproduce our results every time we run them. However, it makes it impossible to predict what are exact results will be if we chose a new `random_state`.\n",
    "\n",
    "`random_state` is very useful for testing that your model was made correctly since it provides you with the same split each time. However, make sure you remove it if you are testing for model variability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "1399  1400          50       RL         51.0     6171   Pave   NaN      Reg   \n",
      "\n",
      "     LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
      "1399         Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
      "\n",
      "     MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "1399         NaN       0     10    2009        WD         Normal  \n",
      "\n",
      "[1 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT a random_state parameter:\n",
    "#  (If you run this code several times, you get different results!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Print the first row of X_Train.\n",
    "print(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
      "6   7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC Fence MiscFeature  \\\n",
      "6         Lvl    AllPub  ...           0        0    NaN   NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "6       0      8    2007        WD         Normal  \n",
      "\n",
      "[1 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "# WITH a random_state parameter:\n",
    "#  (Same split every time! Note you can change the random state to any integer.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Print the first row of each X_train.\n",
    "print(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Boston housing dataset from sklearn and print its documentation\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['MEDV'])\n",
    "\n",
    "boston = pd.concat([y, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEDV</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>-0.381626</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.468536</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.737663</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MEDV      CRIM        ZN     INDUS      CHAS       NOX        RM  \\\n",
       "MEDV     1.000000 -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360   \n",
       "CRIM    -0.388305  1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247   \n",
       "ZN       0.360445 -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991   \n",
       "INDUS   -0.483725  0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676   \n",
       "CHAS     0.175260 -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251   \n",
       "NOX     -0.427321  0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188   \n",
       "RM       0.695360 -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000   \n",
       "AGE     -0.376955  0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265   \n",
       "DIS      0.249929 -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246   \n",
       "RAD     -0.381626  0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847   \n",
       "TAX     -0.468536  0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048   \n",
       "PTRATIO -0.507787  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501   \n",
       "B        0.333461 -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069   \n",
       "LSTAT   -0.737663  0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808   \n",
       "\n",
       "              AGE       DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "MEDV    -0.376955  0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  \n",
       "CRIM     0.352734 -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  \n",
       "ZN      -0.569537  0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
       "INDUS    0.644779 -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
       "CHAS     0.086518 -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
       "NOX      0.731470 -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
       "RM      -0.240265  0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
       "AGE      1.000000 -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
       "DIS     -0.747881  1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
       "RAD      0.456022 -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
       "TAX      0.506456 -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
       "PTRATIO  0.261515 -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
       "B       -0.273534  0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
       "LSTAT    0.602339 -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make `X` a DataFrame with columns \"AGE\" and \"RM\" from the Boston housing dataset, and make `y` a series with column \"MEDV.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.loc[:,['AGE', 'RM']]\n",
    "y = boston.loc[:,'MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split `X` and `y` from the previosu step into a training set and a test state, setting a random state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a linear regression model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print mean squared error (not R^2) for both the training set and the test set. You can use the `mean_squared_error` function imported below. Consult the printed documentation if you are not sure how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmultioutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform_average'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Mean squared error regression loss\n",
       "\n",
       "Read more in the :ref:`User Guide <mean_squared_error>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
       "    Estimated target values.\n",
       "\n",
       "sample_weight : array-like of shape = (n_samples), optional\n",
       "    Sample weights.\n",
       "\n",
       "multioutput : string in ['raw_values', 'uniform_average']\n",
       "    or array-like of shape (n_outputs)\n",
       "    Defines aggregating of multiple output values.\n",
       "    Array-like value defines weights used to average errors.\n",
       "\n",
       "    'raw_values' :\n",
       "        Returns a full set of errors in case of multioutput input.\n",
       "\n",
       "    'uniform_average' :\n",
       "        Errors of all outputs are averaged with uniform weight.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "loss : float or ndarray of floats\n",
       "    A non-negative floating point value (the best value is 0.0), or an\n",
       "    array of floating point values, one for each individual target.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import mean_squared_error\n",
       ">>> y_true = [3, -0.5, 2, 7]\n",
       ">>> y_pred = [2.5, 0.0, 2, 8]\n",
       ">>> mean_squared_error(y_true, y_pred)\n",
       "0.375\n",
       ">>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
       ">>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
       ">>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS\n",
       "0.708...\n",
       ">>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
       "... # doctest: +ELLIPSIS\n",
       "array([0.41666667, 1.        ])\n",
       ">>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
       "... # doctest: +ELLIPSIS\n",
       "0.825...\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_squared_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.776240601175154"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write down the equation that characterizes your fitted model. (*Hint*: use the relevant methods to print the model's intercept and coefficients, and then insert those parameters into the relevant linear regression equation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22.471287589183476"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(['AGE', 'RM'],lr.coef_))\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = -22.47 -0.08*AGE + 7.99*RM$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BONUS:** See if you can get a better score on the test set by using a different set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.47827319768052"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = boston.loc[:,['TAX', 'RM', 'LSTAT']]\n",
    "y = boston.loc[:,'MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 10)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## $K$-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that if you repeat the process of doing a train/test split and evaluating performance on the test set without setting the random state, you do not get the same score every time. If your dataset is not large or if it contains outliers, then the scores you get might vary a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7300796660149407\n",
      "0.7522164587874405\n"
     ]
    }
   ],
   "source": [
    "# Run this cell repeatedly to see how the scores can vary.\n",
    "feature_cols = ['GarageCars',\n",
    "                'GrLivArea',\n",
    "                'OverallQual'\n",
    "               ]\n",
    "X = ames_df.loc[:, feature_cols]\n",
    "y = ames_df.loc[:, 'SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-fold cross validation helps address this problem by doing multiple train/test splits and averaging the results. For instance, five-fold cross validation consists of doing five train/test splits, each time holding out a different one-fifth of the rows, and averaging the scores over those test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/images/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would generally use $k$-folds cross validation to develop your model and then retrain that model on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pro:** $K$-fold cross-validation provides more reliable estimates of model performance than a simple train/test split, especially for small datasets.\n",
    "- **Con:** $K$-fold cross-validation takes longer to run because it requires training and running the model $k$ times instead of just once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Varying K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing $k$ augments both the advantages and disadvantages of $k$-fold cross-validation over a simple train/test split.\n",
    "\n",
    "The extreme case of k-fold cross-validation is leave-one-out cross-validation, where you hold out just one data point at a time. This approach provides the best estimates of model perforamnce, but is requires training the model once for every row in the dataset.\n",
    "\n",
    "A more common approach is to set $k$ somewhere between 5 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation With the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides a single function `cross_val_score` that does all of the work of doing cross-validation splits, training the model on each training set, and scoring it on the corresponding test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise-deprecating'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Evaluate a score by cross-validation\n",
       "\n",
       "Read more in the :ref:`User Guide <cross_validation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : estimator object implementing 'fit'\n",
       "    The object to use to fit the data.\n",
       "\n",
       "X : array-like\n",
       "    The data to fit. Can be for example a list, or an array.\n",
       "\n",
       "y : array-like, optional, default: None\n",
       "    The target variable to try to predict in the case of\n",
       "    supervised learning.\n",
       "\n",
       "groups : array-like, with shape (n_samples,), optional\n",
       "    Group labels for the samples used while splitting the dataset into\n",
       "    train/test set.\n",
       "\n",
       "scoring : string, callable or None, optional, default: None\n",
       "    A string (see model evaluation documentation) or\n",
       "    a scorer callable object / function with signature\n",
       "    ``scorer(estimator, X, y)``.\n",
       "\n",
       "cv : int, cross-validation generator or an iterable, optional\n",
       "    Determines the cross-validation splitting strategy.\n",
       "    Possible inputs for cv are:\n",
       "\n",
       "    - None, to use the default 3-fold cross validation,\n",
       "    - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
       "    - :term:`CV splitter`,\n",
       "    - An iterable yielding (train, test) splits as arrays of indices.\n",
       "\n",
       "    For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
       "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
       "    other cases, :class:`KFold` is used.\n",
       "\n",
       "    Refer :ref:`User Guide <cross_validation>` for the various\n",
       "    cross-validation strategies that can be used here.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "        ``cv`` default value if None will change from 3-fold to 5-fold\n",
       "        in v0.22.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of CPUs to use to do the computation.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "verbose : integer, optional\n",
       "    The verbosity level.\n",
       "\n",
       "fit_params : dict, optional\n",
       "    Parameters to pass to the fit method of the estimator.\n",
       "\n",
       "pre_dispatch : int, or string, optional\n",
       "    Controls the number of jobs that get dispatched during parallel\n",
       "    execution. Reducing this number can be useful to avoid an\n",
       "    explosion of memory consumption when more jobs get dispatched\n",
       "    than CPUs can process. This parameter can be:\n",
       "\n",
       "        - None, in which case all the jobs are immediately\n",
       "          created and spawned. Use this for lightweight and\n",
       "          fast-running jobs, to avoid delays due to on-demand\n",
       "          spawning of the jobs\n",
       "\n",
       "        - An int, giving the exact number of total jobs that are\n",
       "          spawned\n",
       "\n",
       "        - A string, giving an expression as a function of n_jobs,\n",
       "          as in '2*n_jobs'\n",
       "\n",
       "error_score : 'raise' | 'raise-deprecating' or numeric\n",
       "    Value to assign to the score if an error occurs in estimator fitting.\n",
       "    If set to 'raise', the error is raised.\n",
       "    If set to 'raise-deprecating', a FutureWarning is printed before the\n",
       "    error is raised.\n",
       "    If a numeric value is given, FitFailedWarning is raised. This parameter\n",
       "    does not affect the refit step, which will always raise the error.\n",
       "    Default is 'raise-deprecating' but from version 0.22 it will change\n",
       "    to np.nan.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "scores : array of float, shape=(len(list(cv)),)\n",
       "    Array of scores of the estimator for each run of the cross validation.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import datasets, linear_model\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> diabetes = datasets.load_diabetes()\n",
       ">>> X = diabetes.data[:150]\n",
       ">>> y = diabetes.target[:150]\n",
       ">>> lasso = linear_model.Lasso()\n",
       ">>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS\n",
       "[0.33150734 0.08022311 0.03531764]\n",
       "\n",
       "See Also\n",
       "---------\n",
       ":func:`sklearn.model_selection.cross_validate`:\n",
       "    To run cross-validation on multiple metrics and also to return\n",
       "    train scores, fit times and score times.\n",
       "\n",
       ":func:`sklearn.model_selection.cross_val_predict`:\n",
       "    Get predictions from each split of cross-validation for diagnostic\n",
       "    purposes.\n",
       "\n",
       ":func:`sklearn.metrics.make_scorer`:\n",
       "    Make a scorer from a performance metric or loss function.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_df = pd.read_csv('../assets/data/ames_train.csv')\n",
    "ames_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7349459032805817"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use `cross_val_score` to do five-fold cross-validation on the Ames dataset\n",
    "lr = LinearRegression()\n",
    "\n",
    "feature_cols = ['GarageCars',\n",
    "                'GrLivArea',\n",
    "                'OverallQual'\n",
    "               ]\n",
    "X = ames_df.loc[:, feature_cols]\n",
    "y = ames_df.loc[:, 'SalePrice']\n",
    "\n",
    "scores = cross_val_score(estimator=lr, X=X, y=y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_score` allows you to pass a cross-validation splitter object to the `cv` argument instead of an integer for greater control over how the splits are done. For instance, it is often a good idea to shuffle your rows before doing cross-validation in case something changes about the data as you move down the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7298181315351785"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a K-fold object with `shuffle=True` to ensure that results aren't\n",
    "# biased by ordering effects.\n",
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=5,shuffle=True)\n",
    "scores = cross_val_score(estimator=lr,X=X,y=y, cv=kf)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `cross_val_score` does not allow us to assess whether a model is overfitting or underfitting because it returns only test-set scores. To get training set scores as well, we need to write our own cross-validation scoring loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7393411982974599\n",
      "0.7358577811805931\n"
     ]
    }
   ],
   "source": [
    "# Write our own cross-validation loop.\n",
    "# Run this cell a few times to see how much more stable the scores are\n",
    "# than they were with a simple train/test split.\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "kf =KFold(n_splits=5,shuffle=True)\n",
    "for train_rows,test_rows in kf.split(X,y):\n",
    "    X_train = X.iloc[train_rows,:]\n",
    "    y_train = y.iloc[train_rows]\n",
    "    X_test = X.iloc[test_rows,:]\n",
    "    y_test = y.iloc[test_rows]\n",
    "    \n",
    "    lr=LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    train_scores.append(lr.score(X_train,y_train))\n",
    "    test_scores.append(lr.score(X_test,y_test))\n",
    "    \n",
    "print(np.array(train_scores).mean())\n",
    "print(np.array(test_scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits for Time-Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time-series modeling, you are predicting future values of a variable such as a stock price from past values of that variable (perhaps along with other inputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of a train-test split is to simulate what will happen when you deploy the model. As a result, **in a useful train-test split for time series data, all of the training data comes from before all of the test data.** After all, when the model is deployed it will be used to predict future observations based on past observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many decisions to make in doing train/test splits for time series models (sometimes called *backtesting*:\n",
    "\n",
    "- How much of a \"burn-in period\" will you allow before you start scoring the model?\n",
    "- Will you retrain the model periodically after you start scoring it? If so, when? On all past data, or on a \"rolling window\" of just the most recent data?\n",
    "- When you make a prediction for scoring, will you predict just one time point into the future, or multiple time points? Will you use all past data as inputs to the prediction, or just a rolling window of the most recent data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key principle to keep in mind in all of these decisions is that **the purpose of applying the model to test data is to simulate what will happen when you deploy it**. As a result, you should make test conditions mimic what will happen in production as much as is feasible, erring on the side of making the test more difficult so that if anything you underpromise on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Summary\n",
    "\n",
    "- $K$-folds cross-validation consists of doing $k$ train/test splits with disjoint test sets and averaging the resulting test-set metrics. It takes longer to run than a simple train/test split but provides more reliable estimates of test-set performance.\n",
    "- Doing proper train/test splits for time-series models is tricky. You need to make sure that all of the training data comes from earlier in time than all of the test-set data and in general to simulate how the model will be used in production."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
