{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Evaluating Binary Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Limitations of Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Can Be Misleading with Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.score` method of a scikit-learn classification estimator returns *accuracy*, which is simply the proportion of predictions that are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit    gre   gpa  prestige\n",
       "0      0  380.0  3.61       3.0\n",
       "1      1  660.0  3.67       3.0\n",
       "2      1  800.0  4.00       1.0\n",
       "3      1  640.0  3.19       4.0\n",
       "4      0  520.0  2.93       4.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load admissions data\n",
    "# /scrub/\n",
    "admissions_path = '../assets/data/admissions.csv'\n",
    "admissions = pd.read_csv(admissions_path).dropna()\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into feature columns and target column\n",
    "# /scrub/\n",
    "feature_cols = ['gre']\n",
    "target_col = 'admit'\n",
    "X = admissions.loc[:, feature_cols]\n",
    "y = admissions.loc[:, target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train/test split\n",
    "# /scrub/\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a logistic regression estimator\n",
    "# /scrub/\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate test-set accuracy\n",
    "# /scrub/\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64% accuracy might sound OK, but check the class frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.64\n",
       "1    0.36\n",
       "Name: admit, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class frequencies\n",
    "# /scrub/\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (4 mins., in groups)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the model's predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the model doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Predicting \"no\" every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the model's accuracy on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "64%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the model's accuracy on students who are admitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we applied this model to an extremely selective program that admitted only 5% of applicants. What accuracy score would it get, assuming that it behaves the same way in that sample as it does on our test set? Does that mean that it is a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "It would get an accuracy score of 95%. That score does not mean that it is a good model -- we don't need machine learning to get 95% accuracy by guessing \"no\" every time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of accuracy as an evaluation metric is that **it can be misleading when classes are imbalanced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Does Not Weigh the Costs of Different Kinds of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are building an airport security model that flags people for additional scrutiny when they are suspected of carrying dangerous items. Suppose that 1 out every 100 are carrying potentially dangerous items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 raises an alert in 90% of the cases in which a person is carrying a potentially dangerous item and in 10% of the cases in which a person is not carrying a potentially dangerous item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 raises an alert in 10% of the cases in which a person is carrying a potentially dangerous item and in 5% of the cases in which a person is not carrying a potentially dangerous item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise. (5 mins., in groups)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Model 1's accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Model 2's accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    ".1*.01+.95*.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which model would you recommend using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "I would recommend using Model 1. Model 2 is more accurate overall, but Model 1 is much more accurate for people who are carrying dangerous items, and the cost of being wrong in those cases is much higher than the cost of being wrong for people who are not carrying dangerous weapons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second limitation of accuracy is that **it does not take into account the relative costs of false positives and false negatives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *confusion matrix* shows the counts for all combinations of true labels and predictions for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64,  0],\n",
       "       [36,  0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the confusion matrix for our admissions model\n",
    "# /scrub/\n",
    "y_pred = lr.predict(X_test)\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn uses columns for the predicted class, increasing from left to right, so that the first column is the predicted negative \"0\" class (not admitted) and the second is the predicted positive \"1\" class (admitted).\n",
    "\n",
    "It uses rows for the true class, increasing from top to bottom, so that the first row is the actual negative \"0\" class (not admitted) and the second is the actual positive \"1\" class (admitted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (8 mins., in groups)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call it a \"true positive\" when the model correctly predicts that someone is in the positive class (admitted). How many true positives did our model generate in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call it a \"true negative\" when the model correctly predicts that someone is in the negative class (not admitted). How many true negatives did our model generate in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call it a \"false positive\" when the model *incorrectly* predicts that someone is in the positive class (admitted). How many false positives did our model generate in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call it a \"false negative\" when the model *incorrectly* predicts that someone is in the negative class (not admitted). How many false negatives did our model generate in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorize the following cases as true positives, true negatives, false positives, and false negatives.**\n",
    "    \n",
    "- We predict that a growth is malignant, and it is benign. (is_malignant=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We predict that an image does not contain a cat, and it does not. (has_cat=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "TN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We predict that a locomotive will fail in the next two weeks, and it does. (breaks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We predict that a user will like a song, and she does not. (likes_song=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give some examples of scenarios in which a false positive is worse than a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Criminal conviction, scientific discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Give some examples of scenarios in which a false negative is worse than a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Medical screening, preventive maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Probability Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a scikit-learn logistic regression estimator's `predict` method returns 1 exactly where the predicted probability is greater than .5. We can change that. For instance, we might want to lower the threshold for making a positive prediction in our admissions model so that it does not predict 0 every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29218435, 0.31554192, 0.26987406, 0.33987013, 0.33002806,\n",
       "       0.31554192, 0.31554192, 0.31554192, 0.26987406, 0.31079002,\n",
       "       0.35489405, 0.30140557, 0.29218435, 0.32516165, 0.25281932,\n",
       "       0.34484418, 0.25701539, 0.35996808, 0.33002806, 0.24456417,\n",
       "       0.34985245, 0.33493115, 0.29677426, 0.28763639, 0.26554309,\n",
       "       0.30140557, 0.32033269, 0.30607769, 0.29677426, 0.33493115,\n",
       "       0.29218435, 0.29677426, 0.33987013, 0.34484418, 0.30140557,\n",
       "       0.31554192, 0.28763639, 0.30140557, 0.35996808, 0.26125675,\n",
       "       0.2831309 , 0.27866838, 0.33493115, 0.31554192, 0.31079002,\n",
       "       0.28763639, 0.27866838, 0.30607769, 0.32516165, 0.31554192,\n",
       "       0.32516165, 0.27866838, 0.33493115, 0.29677426, 0.27424929,\n",
       "       0.35996808, 0.30140557, 0.32033269, 0.30607769, 0.31079002,\n",
       "       0.29677426, 0.32033269, 0.32516165, 0.33493115, 0.31079002,\n",
       "       0.29677426, 0.32516165, 0.29218435, 0.31554192, 0.31079002,\n",
       "       0.33002806, 0.30607769, 0.31554192, 0.30140557, 0.29677426,\n",
       "       0.35996808, 0.34985245, 0.35996808, 0.30607769, 0.29677426,\n",
       "       0.26554309, 0.31079002, 0.34484418, 0.2831309 , 0.34985245,\n",
       "       0.31554192, 0.30140557, 0.27424929, 0.35996808, 0.30140557,\n",
       "       0.31554192, 0.33002806, 0.33002806, 0.29218435, 0.33493115,\n",
       "       0.26987406, 0.32033269, 0.25701539, 0.33002806, 0.27424929])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get our model's probability predictions for the positive class\n",
    "# /scrub/\n",
    "y_pred_prob = lr.predict_proba(X_test)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 39],\n",
       "       [ 9, 27]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the confusion matrix for a probability threshold of .3\n",
    "# /scrub/\n",
    "y_pred_low_thresh = y_pred_prob > .3\n",
    "metrics.confusion_matrix(y_test, y_pred_low_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (6 mins., in groups)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many true positives did our model generate with this new threshold probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many true negatives did it generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many false positives did it generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many false negatives did it generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is its accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "52%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a medical screening example where false positives are stressful and inconvenient but false negatives are potentially deadly, would you want to use a probability threshold above or below .5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a criminal trial where false negatives allow guilty people to walk free while false positives put innocent people in prison, would you want to use a probability threshold above or below .5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Precision, Recall, and $F_\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice assistants are generally designed to become active when you say a \"wake word\" or phrase (e.g. \"Alexa,\" \"OK Google,\" or \"Hey, Siri\"). To make these devices work well, their engineers need to classify utterances according to whether or not they are instances of the wake word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model's *precision* is the accuracy of its positive predictions -- for instance, when the voice assistant becomes active, how often was it because someone actually said the wake word? In other words, precision measures **how good a model is at avoiding false positives**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model's *recall* is its accuracy on the positive class -- when someone says the wake word, how often does the voice assistant respond? In other words, recall measures **how good a model is at avoiding false negatives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, suppose a wake word model generates the following confusion matrix.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"\"><b> </b></td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center; color: blue\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center; color: green\">TP = 100</td>\n",
    "    <td style=\"text-align: center; color: orange\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center; color: red\">110</td>\n",
    "    <td style=\"border: none; color: purple\">165</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy is\n",
    "\n",
    "$$\\frac{|\\color{green}{\\text{True Positives}}| + |\\color{blue}{\\text{True Negatives}}|}{|\\color{purple}{\\text{Total}}|} = \\frac{50 + 100}{165} = \\frac{150}{165} = .91$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's precision is\n",
    "\n",
    "$$\\frac{|\\color{green}{\\text{True Positives}}|}{|\\color{red}{\\text{Positive Predictions}}|} = \\frac{100}{110} = .91$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's recall is\n",
    "\n",
    "$$\\frac{|\\color{green}{\\text{True Positives}}|}{|\\color{orange}{\\text{Positive Class}}|} = \\frac{100}{105} = .95$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (6 mins., in groups)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain what the precision reported above means. A voice assistant that uses this model will do what 91% of the time under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "It will stay asleep for 91% of utterances that are not the wake word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain what the recall reported above means. A voice assistant that uses this model will do what 95% of the time under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "It will wake up 95% of the time when the wake word is uttered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here again is the confusion matrix for our admissions model with a probability threshold of .3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25, 39],\n",
       "       [ 9, 27]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_low_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is this model's precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4090909090909091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "27/(27 + 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does that number mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "When the model predicts that a student will be admitted, it is right 41% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is this model's recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "27/(27 + 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does that number mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "The model made the correct prediction for 75% of the students who were admitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens to a logistic regression model's precision and recall as you decrease its threshold probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Recall increases, precision decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F_\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with accuracy is that it does not account for the relative costs of false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address this problem by using precision and recall to measure the model's ability to avoid false positives and false negatives, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have two numbers instead of one. If we want to decide how to set our probability threshold or more generally what model to use, **we need to weigh precision against recall somehow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The $F_\\beta$ score addresses this problem.** It is a \"harmonic mean\" of precision and recall, which is similar to the standard arithmetic mean but closer to the minimum. As a result, a model will have a low $F_\\beta$ score if either precision or recall is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\beta$ in $F_\\beta$ encodes how much you care about recall relative to precision: $\\beta=2$, for instance, encodes that you care twice as much about recall as your care about precision, and $\\beta=1/2$ encodes the opposite. **The $\\beta$ parameter allows you to account for the relative costs of false positives and false negatives.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.2445641739351176\n",
      "precision: 0.36363636363636365\n",
      "recall: 1.0\n",
      "f1: 0.5333333333333333\n",
      "f2: 0.7407407407407408\n",
      "f1/2: 0.41666666666666674\n",
      "\n",
      "threshold: 0.2830321419054502\n",
      "precision: 0.40476190476190477\n",
      "recall: 0.9444444444444444\n",
      "f1: 0.5666666666666667\n",
      "f2: 0.7456140350877193\n",
      "f1/2: 0.4569892473118279\n",
      "\n",
      "threshold: 0.3215001098757828\n",
      "precision: 0.59375\n",
      "recall: 0.5277777777777778\n",
      "f1: 0.5588235294117648\n",
      "f2: 0.5397727272727273\n",
      "f1/2: 0.5792682926829269\n",
      "\n",
      "threshold: 0.3599680778461154\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f1: 0.0\n",
      "f2: 0.0\n",
      "f1/2: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, F_1, F_2, and F_{1/2} for our admissions model\n",
    "# at different thresholds.\n",
    "from sklearn import metrics\n",
    "\n",
    "for threshold in np.linspace(y_pred_prob.min(), y_pred_prob.max(), 4):\n",
    "    y_pred_thresh = y_pred_prob > threshold\n",
    "    print('threshold:', threshold)\n",
    "    print('precision:', metrics.precision_score(y_test, y_pred_thresh))\n",
    "    print('recall:', metrics.recall_score(y_test, y_pred_thresh))\n",
    "    print('f1:', metrics.f1_score(y_test, y_pred_thresh))\n",
    "    print('f2:', metrics.fbeta_score(y_test, y_pred_thresh, 2))\n",
    "    print('f1/2:', metrics.fbeta_score(y_test, y_pred_thresh, .5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using $F_\\beta$ to Tune the Probability Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we care equally about precision and recall in the admissions example, so that $F_1$ is an appropriate evaluation metric. Let's find the probability threshold that maximizes that metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2742492919202762"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the probability threshold that gives the best F-beta score.\n",
    "# Predictions only change when the threshold hits a probability value\n",
    "# that the model actually generates, so we can just check those values.\n",
    "# /scrub/\n",
    "best_score = -1\n",
    "best_threshold = -1\n",
    "\n",
    "for threshold in sorted(set(y_pred_prob)):\n",
    "    y_pred_thresh = y_pred_prob > threshold\n",
    "    score = metrics.f1_score(y_test, y_pred_thresh)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 51],\n",
       "       [ 0, 36]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix for the best threshold.\n",
    "# /scrub/\n",
    "y_pred_thresh = y_pred_prob > best_threshold\n",
    "metrics.confusion_matrix(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (15 mins., in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate this model's accuracy both with base Python operators and with sklearn's `accuracy_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "(13 + 36) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.accuracy_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate this model's precision both with base Python operators and with the appropriate sklearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41379310344827586"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "36/87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41379310344827586"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.precision_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate this model's recall both with base Python operators and with the appropriate sklearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "36/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.recall_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate this model's F1 score with the appropriate sklearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5853658536585366"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.f1_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a medical screening example where false positives are stressful and inconvenient but false negatives are potentially deadly, would you want to use a value for $\\beta$ above or below 1 in an $F_\\beta$ score? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Above, because you care about recall more than precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In a criminal trial where false negatives allow guilty people to walk free while false positives put innocent people in prison, would you want to use a value for $\\beta$ above or below 1 in an $F_\\beta$ score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Below, because you care about precision more than recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the probability threshold that optimizes the $F_2$ score for the titanic model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('../assets/data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.select_dtypes(['int64', 'float64']).dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Survived'\n",
    "X = titanic.drop(target_col, axis='columns')\n",
    "y = titanic.loc[:, target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = lr.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2570949367160027"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "best_score = -1\n",
    "best_threshold = -1\n",
    "\n",
    "for threshold in sorted(set(y_pred_prob)):\n",
    "    y_pred_thresh = y_pred_prob > threshold\n",
    "    score = metrics.fbeta_score(y_test, y_pred_thresh, 2)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the test-set accuracy, precision, recall, and $F_2$ score for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /scrub/\n",
    "y_pred_thresh = y_pred_prob > best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6188340807174888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.accuracy_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.precision_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9213483146067416"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.recall_score(y_test, y_pred_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7945736434108527"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /scrub/\n",
    "metrics.fbeta_score(y_test, y_pred_thresh, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Lesson Review\n",
    "\n",
    "- Accuracy scores can be misleading when classes are imbalanced, and they do not account for the relative costs of false positives and false negatives.\n",
    "- A confusion matrix displays the number of false positives, false negatives, true positives, and true negatives a model generates.\n",
    "- Precision measures a model's ability to avoid false positives.\n",
    "- Recall measures a model's ability to avoid false negatives.\n",
    "- The $F_\\beta$ score allows you to weigh precision against recall to select the best overall model even when the costs of false positives and false negatives are unequal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
