{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Natural Language Processing (NLP)?\n",
    "\n",
    "Using computers to process (analyze, understand, generate) natural human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use NLP?\n",
    "\n",
    "An enormous amount of information is stored as text. Computers can process this information much faster than humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Level NLP Tasks\n",
    "\n",
    "- **Chatbots:** Understand natural language from the user and return intelligent responses.\n",
    "    - [Api.ai](https://api.ai/)\n",
    "- **Information retrieval:** Find relevant results and similar results.\n",
    "    - [Google](https://www.google.com/)    \n",
    "- **Information extraction:** Structured information from unstructured documents.\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation:** One language to another.\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification:** Preserve the meaning of text, but simplify the grammar and vocabulary.\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input:** Faster or easier typing.\n",
    "    - [Phrase completion application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis:** Attitude of speaker.\n",
    "    - [Hater News](https://medium.com/@KevinMcAlear/building-hater-news-62062c58325c)\n",
    "- **Automatic summarization:** Extractive or abstractive summarization.\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural language generation:** Generate text from data.\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation:** Speech-to-text, text-to-speech.\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering:** Determine the intent of the question, match query with knowledge base, evaluate hypotheses.\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower-Level Components\n",
    "\n",
    "- **Tokenization:** Breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stop-word removal:** a/an/the\n",
    "- **Stemming and lemmatization:** root word\n",
    "- **TF-IDF:** word importance\n",
    "- **Part-of-speech tagging:** noun/verb/adjective\n",
    "- **Named entity recognition:** person/organization/location\n",
    "- **Spelling correction:** \"New Yrok City\"\n",
    "- **Word sense disambiguation:** \"buy a mouse\"\n",
    "- **Segmentation:** \"New York City subway\"\n",
    "- **Language detection:** \"translate this page\"\n",
    "- **Vectorizing:** Turning documents into vectors of numbers for use in machine learning\n",
    "- **Machine learning:** specialized models that work well with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why NLP is hard\n",
    "\n",
    "Natural language processing requires an understanding of the language and the world. Several limitations of NLP are:\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals Are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English:** text messages\n",
    "- **Idioms:** \"throw in the towel\"\n",
    "- **Newly coined words:** \"retweet\"\n",
    "- **Tricky entity names:** \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge:** \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP terms\n",
    "\n",
    "- **corpus** (plural **corpora**): a collection of documents (derived from the Latin word for \"body\")\n",
    "- **document**: any item in a corpus (e.g. email, book chapter, tweet, article, or text message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_rev'></a>\n",
    "\n",
    "# Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "path = Path('..', 'assets', 'data', 'yelp.csv')\n",
    "yelp = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify star rating prediction problem by making it binary, splitting between 3 and 4\n",
    "# /scrub/\n",
    "yelp.loc[:, 'positive_rating'] = yelp.loc[:, 'stars'].map({1:0, 2:0, 3:0, 4:1, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "# /scrub/\n",
    "X = yelp.loc[:, 'text']\n",
    "y = yelp.loc[:, 'positive_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the new DataFrame into training and testing sets.\n",
    "# /scrub/\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>positive_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  positive_rating  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0                1  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0                1  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0                1  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0                1  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The head of the data\n",
    "# /scrub/\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text_class'></a>\n",
    "\n",
    "\n",
    "# Introduction: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "E.g.:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning text into feature vectors**\n",
    "\n",
    "The only difference between this task and the kinds of classification tasks we have been considering is that our inputs consist of text rather than numeric features.\n",
    "\n",
    "If we can find a way to represent text using a set of numeric features, then we can use standard machine learning classifiers for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start out with a **bag-of-words representation:**\n",
    "\n",
    "- Preprocess the text, e.g. to remove punctuation and convert uppercase letters to lowercase.\n",
    "- Create a vocabulary, e.g. every word in the corpus.\n",
    "- Make each word in the vocabulary a feature.\n",
    "- Represent each document with a vector that indicates how many times each word in the vocabulary appears in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Text Processing in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vec'></a>\n",
    "### Creating Features Using CountVectorizer\n",
    "\n",
    "- **What:** Converts each document into a set of words and their counts.\n",
    "- **Why:** To use a machine learning model, we must convert unstructured text into numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='countvectorizer-model'></a>\n",
    "\n",
    "\n",
    "### Using CountVectorizer in a Model\n",
    "![DTM](../assets/images/DTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on bag-of-words:**\n",
    "\n",
    "- The phrases \"term-document matrix\" and \"document-term matrix\" are interchangeable.\n",
    "- Vocabulary will often contain tens of thousands of words or more.\n",
    "- Most features will have a value of zero for most documents, resulting in a sparse matrix of features.\n",
    "- This approach is called \"bag-of-words\" because it loses the document's structure — as if the words are all jumbled up in a bag.\n",
    "- Rather than counting occurrences of each word, you might just record a 1 or 0 to indicate whether it is present or divide by the length of the document to indicate the word's frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test.\n",
    "# /scrub/\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7500x25797 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 622700 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformed feature matrices are stored as sparse matrices for efficiency.\n",
    "\n",
    "# A sparse representation stores the vaues and locations of non-zero elements,\n",
    "# rather than storing a number for every element, which saves space when\n",
    "# most elements are zero.\n",
    "# /scrub/\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"sparse matrix\" data structure records which positions in the matrix have nonzero values and what those values are, as opposed to a \"dense matrix\" data structure that records the value at every position. The sparse matrix format is much more space-efficient when the matrix consists primarily of zeros, as a typical document-term matrix does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View as a dense matrix\n",
    "# /scrub/\n",
    "X_train_dtm.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 25797)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n",
    "# /scrub/\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (3 mins., post right away)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7500 of what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 25797 of what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Distinct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you interpret the output of `X_train_dtm.sum(axis='columns')`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Words in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you interpret the output of `X_train_dtm.sum(axis='index')`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Occurrences of each word in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zillion', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipper', 'zippers', 'zipps', 'zippy', 'ziti', 'zoe', 'zoey', 'zoftik', 'zola', 'zombie', 'zombies', 'zone', 'zoned', 'zoners', 'zones', 'zoning', 'zoo', 'zoom', 'zoomed', 'zoos', 'zoyo', 'zpizza', 'zu', 'zucca', 'zucchini', 'zuccini', 'zuch', 'zuchinni', 'zuma', 'zumba', 'zupa', 'zur', 'zuzu', 'zuzus', 'zweigel', 'zwiebel', 'zy', 'zzed', 'zzzzzzzzzzzzzzzzz', 'école', 'òc']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "# /scrub/\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'having': 10754,\n",
       " 'lived': 13443,\n",
       " 'in': 11649,\n",
       " 'chicago': 4401,\n",
       " 'for': 9216,\n",
       " 'most': 14935,\n",
       " 'of': 15765,\n",
       " 'my': 15116,\n",
       " 'life': 13287,\n",
       " 'was': 24893,\n",
       " 'more': 14895,\n",
       " 'than': 23005,\n",
       " 'nervous': 15333,\n",
       " 'what': 25080,\n",
       " 'an': 1238,\n",
       " 'arizona': 1577,\n",
       " 'pizza': 17145,\n",
       " 'experience': 8353,\n",
       " 'would': 25428,\n",
       " 'be': 2332,\n",
       " 'like': 13321,\n",
       " 'the': 23029,\n",
       " 'part': 16507,\n",
       " 'assumptions': 1741,\n",
       " 'were': 25045,\n",
       " 'correct': 5560,\n",
       " 'but': 3578,\n",
       " 'grimaldi': 10304,\n",
       " 'is': 12168,\n",
       " 'great': 10249,\n",
       " 'we': 24966,\n",
       " 'get': 9859,\n",
       " 'margarita': 14013,\n",
       " 'and': 1262,\n",
       " 'it': 12187,\n",
       " 'amazing': 1170,\n",
       " 'normally': 15539,\n",
       " 'deep': 6380,\n",
       " 'dish': 6941,\n",
       " 'person': 16848,\n",
       " 'slowly': 20929,\n",
       " 'winning': 25276,\n",
       " 'me': 14258,\n",
       " 'over': 16142,\n",
       " 'thin': 23088,\n",
       " 'crust': 5941,\n",
       " 'also': 1124,\n",
       " 'this': 23112,\n",
       " 'place': 17163,\n",
       " 'has': 10713,\n",
       " 'cheesecake': 4346,\n",
       " 'best': 2548,\n",
       " 've': 24480,\n",
       " 'ever': 8191,\n",
       " 'had': 10527,\n",
       " 'restaurant': 18998,\n",
       " 'clean': 4740,\n",
       " 'facility': 8488,\n",
       " 'prices': 17715,\n",
       " 'are': 1550,\n",
       " 'super': 22303,\n",
       " 'low': 13654,\n",
       " 'you': 25641,\n",
       " 'can': 3764,\n",
       " 'go': 10021,\n",
       " 'wong': 25363,\n",
       " 'with': 25309,\n",
       " 'saving': 19812,\n",
       " 'money': 14832,\n",
       " 'absolute': 567,\n",
       " 'favorite': 8648,\n",
       " 'phoenix': 16945,\n",
       " 'where': 25105,\n",
       " 'to': 23317,\n",
       " 'celebrate': 4109,\n",
       " 'enjoy': 7929,\n",
       " 'night': 15415,\n",
       " 'out': 16091,\n",
       " 'food': 9181,\n",
       " 'service': 20244,\n",
       " 'noca': 15479,\n",
       " 'wouldn': 25430,\n",
       " 'recommend': 18519,\n",
       " 'trying': 23761,\n",
       " 'quickly': 18165,\n",
       " 'hit': 11062,\n",
       " 'up': 24280,\n",
       " 'meal': 14260,\n",
       " 'here': 10942,\n",
       " 'before': 2422,\n",
       " 'catch': 4047,\n",
       " 'movie': 14987,\n",
       " 'imagine': 11562,\n",
       " 'rush': 19522,\n",
       " 'on': 15876,\n",
       " 'our': 16086,\n",
       " 'recent': 18474,\n",
       " 'visit': 24689,\n",
       " 'immediately': 11576,\n",
       " 'ordered': 15994,\n",
       " 'cocktail': 4904,\n",
       " 'dirty': 6860,\n",
       " 'bird': 2673,\n",
       " 'so': 21111,\n",
       " 'perfectly': 16807,\n",
       " 'crafted': 5733,\n",
       " 'smooth': 21014,\n",
       " 'plus': 17286,\n",
       " 'there': 23058,\n",
       " 'bleu': 2793,\n",
       " 'cheese': 4343,\n",
       " 'stuffed': 22084,\n",
       " 'olives': 15844,\n",
       " 'sold': 21173,\n",
       " 'felt': 8717,\n",
       " 'little': 13438,\n",
       " 'bad': 2046,\n",
       " 'because': 2380,\n",
       " 'fiance': 8766,\n",
       " 'wanted': 24854,\n",
       " 'new': 15354,\n",
       " 'jobby': 12404,\n",
       " 'jobs': 12407,\n",
       " 'prosecco': 17902,\n",
       " 'lay': 13076,\n",
       " 'off': 15766,\n",
       " 'when': 25103,\n",
       " 'vodka': 24711,\n",
       " 'involved': 12110,\n",
       " 'mister': 14695,\n",
       " 'first': 8907,\n",
       " 'mixed': 14715,\n",
       " 'salad': 19614,\n",
       " 'love': 13637,\n",
       " 'how': 11321,\n",
       " 'not': 15565,\n",
       " 'overloaded': 16190,\n",
       " 'candied': 3782,\n",
       " 'fruit': 9514,\n",
       " 'perfect': 16804,\n",
       " 'balance': 2102,\n",
       " 'flavors': 9007,\n",
       " 'then': 23046,\n",
       " 'lobster': 13483,\n",
       " 'roll': 19347,\n",
       " 'absolutely': 568,\n",
       " 'heavenly': 10851,\n",
       " 'really': 18423,\n",
       " 'hated': 10728,\n",
       " 'share': 20343,\n",
       " 'that': 23019,\n",
       " 'needed': 15286,\n",
       " 'save': 19806,\n",
       " 'room': 19379,\n",
       " 'scallops': 19846,\n",
       " 'cooked': 5474,\n",
       " 'plating': 17213,\n",
       " 'gorgeous': 10118,\n",
       " 'outstanding': 16132,\n",
       " 'sublime': 22134,\n",
       " 'ryan': 19546,\n",
       " 'actually': 716,\n",
       " 'asked': 1684,\n",
       " 'if': 11521,\n",
       " 'remembered': 18782,\n",
       " 'his': 11055,\n",
       " 'name': 15163,\n",
       " 'just': 12540,\n",
       " 'smiled': 20991,\n",
       " 'taste': 22727,\n",
       " 'buds': 3425,\n",
       " 'went': 25043,\n",
       " 'crrraazzyyy': 5912,\n",
       " 'finally': 8849,\n",
       " 'made': 13790,\n",
       " 'much': 15013,\n",
       " 'raved': 18365,\n",
       " 'about': 555,\n",
       " 'pizzeria': 17150,\n",
       " 'bianco': 2602,\n",
       " 'happy': 10659,\n",
       " 'did': 6737,\n",
       " 'delicious': 6463,\n",
       " 'thankfully': 23011,\n",
       " 'at': 1763,\n",
       " 'good': 10079,\n",
       " 'time': 23248,\n",
       " 'didn': 6740,\n",
       " 'have': 10748,\n",
       " 'wait': 24791,\n",
       " 'all': 1065,\n",
       " 'table': 22562,\n",
       " 'yes': 25601,\n",
       " 'margherita': 14023,\n",
       " 'deliciously': 6464,\n",
       " 'pleased': 17243,\n",
       " 'however': 11325,\n",
       " 'sister': 20740,\n",
       " 'opted': 15960,\n",
       " 'biacoverde': 2595,\n",
       " 'boooy': 3003,\n",
       " 'jealous': 12311,\n",
       " 'awesome': 1961,\n",
       " 'by': 3628,\n",
       " 'far': 8584,\n",
       " 'open': 15926,\n",
       " 'business': 3559,\n",
       " 'located': 13494,\n",
       " 'canyon': 3826,\n",
       " 'trails': 23554,\n",
       " 'towne': 23510,\n",
       " 'center': 4134,\n",
       " 'along': 1107,\n",
       " 'main': 13844,\n",
       " 'entrance': 7988,\n",
       " 'banks': 2164,\n",
       " 'casual': 4035,\n",
       " 'cheap': 4302,\n",
       " 'yet': 25605,\n",
       " 'decent': 6310,\n",
       " 'quality': 18119,\n",
       " 'sushi': 22397,\n",
       " 'japanese': 12283,\n",
       " 'entrees': 7992,\n",
       " 'excellent': 8253,\n",
       " 'grand': 10182,\n",
       " 'opening': 15930,\n",
       " 'special': 21400,\n",
       " 'one': 15878,\n",
       " 'free': 9381,\n",
       " 'appetizer': 1450,\n",
       " 'spend': 21444,\n",
       " '10': 17,\n",
       " 'spicy': 21466,\n",
       " 'salmon': 19638,\n",
       " 'avocado': 1933,\n",
       " 'rolls': 19352,\n",
       " 'very': 24573,\n",
       " 'crunchy': 5936,\n",
       " 'shrimp': 20556,\n",
       " 'tempura': 22892,\n",
       " 'dressing': 7346,\n",
       " 'included': 11685,\n",
       " 'miso': 14674,\n",
       " 'soup': 21304,\n",
       " 'bit': 2693,\n",
       " 'bland': 2746,\n",
       " 'they': 23072,\n",
       " 'need': 15285,\n",
       " 'add': 731,\n",
       " 'broth': 3335,\n",
       " 'lunch': 13711,\n",
       " 'specials': 21409,\n",
       " 'include': 11684,\n",
       " 'any': 1377,\n",
       " 'bento': 2523,\n",
       " 'boxe': 3110,\n",
       " '25': 216,\n",
       " 'pick': 17001,\n",
       " '45': 326,\n",
       " 'includes': 11686,\n",
       " 'hmmm': 11076,\n",
       " 'quite': 18188,\n",
       " 'sure': 22356,\n",
       " 'whether': 25112,\n",
       " 'ah': 934,\n",
       " 'kai': 12567,\n",
       " 'grill': 10297,\n",
       " 'or': 15976,\n",
       " 'hai': 10541,\n",
       " 'public': 17961,\n",
       " 'records': 18534,\n",
       " 'show': 20532,\n",
       " 'menu': 14391,\n",
       " 'says': 19835,\n",
       " 'days': 6233,\n",
       " 'week': 25000,\n",
       " 'eat': 7595,\n",
       " 'tons': 23390,\n",
       " 'fruits': 9516,\n",
       " 'veggies': 24501,\n",
       " 'nuts': 15653,\n",
       " 'seeds': 20122,\n",
       " 'whole': 25165,\n",
       " 'grains': 10179,\n",
       " 'fish': 8910,\n",
       " 'etc': 8139,\n",
       " 'tuesday': 23789,\n",
       " 'afternoons': 895,\n",
       " 'cheat': 4307,\n",
       " 'look': 13564,\n",
       " 'forward': 9295,\n",
       " 'looking': 13567,\n",
       " 'dined': 6816,\n",
       " 'afternoon': 894,\n",
       " 'kinda': 12740,\n",
       " 'empty': 7853,\n",
       " 'got': 10126,\n",
       " 'attention': 1820,\n",
       " 'from': 9485,\n",
       " 'waitress': 24800,\n",
       " 'who': 25161,\n",
       " 'wonderful': 25355,\n",
       " 'rock': 19313,\n",
       " 'garlic': 9718,\n",
       " 'basil': 2267,\n",
       " 'aolioli': 1398,\n",
       " 'sp': 21342,\n",
       " 'dipping': 6844,\n",
       " 'sauce': 19776,\n",
       " 'okay': 15824,\n",
       " 'flavorful': 9001,\n",
       " 'maybe': 14207,\n",
       " 'marinara': 14041,\n",
       " 'barbecue': 2186,\n",
       " 'decided': 6317,\n",
       " 'inch': 11671,\n",
       " 'butternut': 3600,\n",
       " 'squash': 21627,\n",
       " 'margerita': 14021,\n",
       " 'something': 21214,\n",
       " 'missing': 14683,\n",
       " 'oh': 15802,\n",
       " 'yeah': 25563,\n",
       " 'seriously': 20229,\n",
       " 'do': 7093,\n",
       " 'ask': 1683,\n",
       " 'extra': 8422,\n",
       " 'day': 6227,\n",
       " 'live': 13442,\n",
       " 'expectations': 8338,\n",
       " 'dessert': 6634,\n",
       " 'donut': 7188,\n",
       " 'holes': 11121,\n",
       " 'drizzled': 7382,\n",
       " 'caramel': 3877,\n",
       " 'sprinkled': 21591,\n",
       " 'powdered': 17539,\n",
       " 'sugar': 22216,\n",
       " 'order': 15992,\n",
       " 'huge': 11364,\n",
       " 'no': 15471,\n",
       " 'less': 13220,\n",
       " '30': 251,\n",
       " 'too': 23395,\n",
       " 'husband': 11433,\n",
       " 'scoops': 19946,\n",
       " 'vanilla': 24453,\n",
       " 'ice': 11476,\n",
       " 'cream': 5792,\n",
       " 'after': 890,\n",
       " 'visiting': 24691,\n",
       " 'local': 13487,\n",
       " 'shop': 20484,\n",
       " 'everything': 8205,\n",
       " 'homemade': 11152,\n",
       " 'come': 5015,\n",
       " 'close': 4813,\n",
       " 'long': 13551,\n",
       " 'story': 21951,\n",
       " 'short': 20494,\n",
       " 'will': 25227,\n",
       " 'again': 902,\n",
       " 'only': 15891,\n",
       " 'next': 15380,\n",
       " 'breakfast': 3199,\n",
       " 'crispy': 5864,\n",
       " 'chicken': 4413,\n",
       " 'fried': 9440,\n",
       " 'steak': 21797,\n",
       " 'eggs': 7703,\n",
       " 'side': 20611,\n",
       " 'texas': 22980,\n",
       " 'served': 20238,\n",
       " 'country': 5646,\n",
       " 'gravy': 10235,\n",
       " 'hash': 10716,\n",
       " 'browns': 3346,\n",
       " 'garnished': 9728,\n",
       " 'slice': 20864,\n",
       " 'cantaloupe': 3817,\n",
       " 'honey': 11179,\n",
       " 'dew': 6688,\n",
       " 'melon': 14355,\n",
       " 'under': 24023,\n",
       " 'green': 10261,\n",
       " 'chili': 4446,\n",
       " 'burger': 3509,\n",
       " 'favorites': 8649,\n",
       " 'down': 7262,\n",
       " 'getting': 9863,\n",
       " 'past': 16554,\n",
       " 'unwashed': 24267,\n",
       " 'morons': 14912,\n",
       " 'gambling': 9675,\n",
       " 'chain': 4179,\n",
       " 'smoking': 21011,\n",
       " 'cigarettes': 4629,\n",
       " 'yum': 25677,\n",
       " 'steaks': 21802,\n",
       " 'corn': 5535,\n",
       " 'bread': 3183,\n",
       " 'large': 13002,\n",
       " 'portions': 17446,\n",
       " 'definitely': 6418,\n",
       " 'worth': 25422,\n",
       " 'price': 17709,\n",
       " 'tried': 23684,\n",
       " 'reading': 18404,\n",
       " 'reviews': 19093,\n",
       " 'rice': 19143,\n",
       " 'noodle': 15515,\n",
       " 'fishballs': 8911,\n",
       " 'squid': 21641,\n",
       " 'hubby': 11347,\n",
       " 'shredded': 20553,\n",
       " 'pork': 17425,\n",
       " 'sandwich': 19714,\n",
       " 'both': 3052,\n",
       " 'plenty': 17255,\n",
       " 'hard': 10671,\n",
       " 'believe': 2460,\n",
       " 'wasn': 24905,\n",
       " 'typo': 23902,\n",
       " 'beats': 2369,\n",
       " 'subway': 22175,\n",
       " 'anyday': 1379,\n",
       " 'neighborhood': 15313,\n",
       " 'shady': 20303,\n",
       " 'don': 7167,\n",
       " 'mind': 14587,\n",
       " 'fine': 8860,\n",
       " 'quick': 18159,\n",
       " 'revisit': 19095,\n",
       " 'people': 16767,\n",
       " 'kept': 12656,\n",
       " 'pouring': 17532,\n",
       " 'during': 7513,\n",
       " 'dinner': 6828,\n",
       " 'hours': 11305,\n",
       " 'beat': 2361,\n",
       " 'truth': 23758,\n",
       " 'easily': 7584,\n",
       " 'spots': 21561,\n",
       " 'sonoran': 21234,\n",
       " 'style': 22109,\n",
       " 'mexican': 14464,\n",
       " 'cuisine': 5983,\n",
       " 'chile': 4441,\n",
       " 'colorado': 4992,\n",
       " 'try': 23760,\n",
       " 'slap': 20838,\n",
       " 'mama': 13906,\n",
       " 'red': 18550,\n",
       " 'chips': 4491,\n",
       " 'brewed': 3242,\n",
       " 'sixth': 20756,\n",
       " 'level': 13236,\n",
       " 'hell': 10897,\n",
       " 'fridays': 9436,\n",
       " 'feature': 8673,\n",
       " '99': 491,\n",
       " 'margaritas': 14014,\n",
       " 'swing': 22497,\n",
       " 'start': 21749,\n",
       " 'crisp': 5857,\n",
       " 'grab': 10148,\n",
       " 'combo': 5013,\n",
       " 'revel': 19079,\n",
       " 'gluttony': 10014,\n",
       " 'eating': 7603,\n",
       " 'pissed': 17117,\n",
       " 'know': 12816,\n",
       " 'sashimi': 19750,\n",
       " 'amazingly': 1171,\n",
       " 'doesn': 7121,\n",
       " 'mean': 14263,\n",
       " 'crap': 5761,\n",
       " 'fortunately': 9290,\n",
       " 'sakana': 19604,\n",
       " 'even': 8182,\n",
       " 'house': 11306,\n",
       " 'smoked': 21004,\n",
       " 'premises': 17624,\n",
       " 'complaint': 5138,\n",
       " 'lack': 12922,\n",
       " 'fresh': 9413,\n",
       " 'wasabi': 24895,\n",
       " 'pickled': 17008,\n",
       " 'stuff': 22083,\n",
       " 'area': 1551,\n",
       " 'should': 20514,\n",
       " 'stars': 21747,\n",
       " 'compare': 5098,\n",
       " 'kiji': 12711,\n",
       " 'sf': 20287,\n",
       " 'blue': 2860,\n",
       " 'ribbon': 19133,\n",
       " 'nyc': 15659,\n",
       " 'which': 25114,\n",
       " 'star': 21732,\n",
       " 'benchmarks': 2498,\n",
       " 'combination': 5007,\n",
       " 'fast': 8616,\n",
       " 'joint': 12431,\n",
       " 'gas': 9733,\n",
       " 'station': 21775,\n",
       " 'yessss': 25602,\n",
       " 'convenient': 5437,\n",
       " 'fill': 8824,\n",
       " 'car': 3873,\n",
       " 'stomach': 21920,\n",
       " 'joking': 12437,\n",
       " 'aside': 1682,\n",
       " 'popeye': 17402,\n",
       " 'isn': 12176,\n",
       " 'pretty': 17692,\n",
       " 'expect': 8334,\n",
       " 'their': 23039,\n",
       " 'workers': 25396,\n",
       " 'seem': 20128,\n",
       " 'nice': 15394,\n",
       " 'let': 13226,\n",
       " 'them': 23041,\n",
       " 'case': 4007,\n",
       " 'while': 25116,\n",
       " 're': 18387,\n",
       " 'waiting': 24797,\n",
       " 'your': 25653,\n",
       " 'pop': 17400,\n",
       " 'store': 21940,\n",
       " 'connected': 5298,\n",
       " 'drive': 7372,\n",
       " 'thru': 23171,\n",
       " 'giving': 9945,\n",
       " 'trainers': 23559,\n",
       " 'itself': 12206,\n",
       " 'overall': 16145,\n",
       " 'truly': 23745,\n",
       " 'trainer': 23558,\n",
       " 'holly': 11137,\n",
       " 'knowledgeable': 12823,\n",
       " 'dogs': 7134,\n",
       " 'general': 9812,\n",
       " 'way': 24955,\n",
       " 'pup': 18033,\n",
       " 'biased': 2607,\n",
       " 'basics': 2266,\n",
       " 'classes': 4717,\n",
       " 'helped': 10908,\n",
       " 'worked': 25393,\n",
       " 'us': 24349,\n",
       " 'make': 13871,\n",
       " 'she': 20367,\n",
       " 'understood': 24058,\n",
       " 'completely': 5146,\n",
       " 'now': 15609,\n",
       " 'petsmart': 16897,\n",
       " 'selection': 20152,\n",
       " 'found': 9305,\n",
       " 'staff': 21675,\n",
       " 'helpful': 10911,\n",
       " 'times': 23256,\n",
       " 'generally': 9815,\n",
       " 'dumb': 7470,\n",
       " 'products': 17812,\n",
       " 'sell': 20160,\n",
       " 'as': 1660,\n",
       " 'well': 25036,\n",
       " 'management': 13921,\n",
       " 'gotten': 10131,\n",
       " 'better': 2567,\n",
       " 'once': 15877,\n",
       " 'kind': 12739,\n",
       " 'nasty': 15211,\n",
       " 'cleaner': 4742,\n",
       " 'its': 12205,\n",
       " 'find': 8856,\n",
       " 'edible': 7643,\n",
       " 'chinese': 4478,\n",
       " 'ones': 15881,\n",
       " 'think': 23095,\n",
       " 'renamed': 18821,\n",
       " 'fuji': 9554,\n",
       " 'buffet': 3433,\n",
       " 'bar': 2178,\n",
       " 'eater': 7598,\n",
       " 'expected': 8339,\n",
       " 'things': 23092,\n",
       " 'kiddos': 12703,\n",
       " 'stay': 21789,\n",
       " 'away': 1956,\n",
       " 'desserts': 6635,\n",
       " 'tho': 23115,\n",
       " 'saying': 19833,\n",
       " 'never': 15349,\n",
       " 'bbq': 2325,\n",
       " 'could': 5623,\n",
       " 'without': 25314,\n",
       " 'today': 23327,\n",
       " 'customer': 6068,\n",
       " 'figured': 8800,\n",
       " 'cool': 5485,\n",
       " 'matter': 14181,\n",
       " 'girl': 9924,\n",
       " 'sat': 19753,\n",
       " 'pleasant': 17239,\n",
       " 'indicated': 11738,\n",
       " 'right': 19194,\n",
       " 'ten': 22893,\n",
       " 'minutes': 14631,\n",
       " 'later': 13034,\n",
       " 'watching': 24921,\n",
       " 'talk': 22635,\n",
       " 'personal': 16850,\n",
       " 'phone': 16951,\n",
       " 'call': 3714,\n",
       " 'came': 3742,\n",
       " 'take': 22616,\n",
       " 'walked': 24818,\n",
       " 'say': 19830,\n",
       " 'word': 25386,\n",
       " 'stared': 21739,\n",
       " 'ok': 15822,\n",
       " 'minor': 14618,\n",
       " 'inconviencence': 11704,\n",
       " 'chopped': 4542,\n",
       " 'beans': 2348,\n",
       " 'lemonade': 13188,\n",
       " 'possibly': 17484,\n",
       " 'bush': 3553,\n",
       " 'tasted': 22730,\n",
       " 'canned': 3800,\n",
       " 'baked': 2093,\n",
       " 'heated': 10842,\n",
       " 'microwave': 14506,\n",
       " 'thinking': 23097,\n",
       " 'back': 2020,\n",
       " 'stacy': 21672,\n",
       " 'bear': 2350,\n",
       " 'other': 16077,\n",
       " 'same': 19678,\n",
       " 'arena': 1555,\n",
       " 'll': 13456,\n",
       " 'though': 23128,\n",
       " 'assumed': 1737,\n",
       " 'looked': 13565,\n",
       " 'watery': 24937,\n",
       " 'thought': 23129,\n",
       " 'whatever': 25082,\n",
       " 'nc': 15251,\n",
       " 'may': 14206,\n",
       " 'been': 2411,\n",
       " 'vinegary': 24651,\n",
       " 'noticed': 15583,\n",
       " 'every': 8197,\n",
       " 'labeled': 12907,\n",
       " 'guess': 10439,\n",
       " 'two': 23889,\n",
       " 'different': 6762,\n",
       " 'preferred': 17613,\n",
       " 'assume': 1736,\n",
       " 'atmosphere': 1783,\n",
       " 'run': 19509,\n",
       " 'appear': 1438,\n",
       " 'remodeled': 18802,\n",
       " 'shish': 20445,\n",
       " 'kabob': 12559,\n",
       " 'vacated': 24405,\n",
       " 'feeling': 8694,\n",
       " 'seeing': 20124,\n",
       " 'customers': 6069,\n",
       " 'assuming': 1739,\n",
       " 'going': 10052,\n",
       " 'lose': 13602,\n",
       " 'act': 699,\n",
       " 'together': 23340,\n",
       " 'fail': 8511,\n",
       " 'superb': 22304,\n",
       " 'average': 1923,\n",
       " 'wine': 25261,\n",
       " 'list': 13414,\n",
       " 'seafood': 20051,\n",
       " 'expertly': 8366,\n",
       " 'sides': 20618,\n",
       " 'tower': 23506,\n",
       " 'put': 18081,\n",
       " 'appropriately': 1498,\n",
       " 'group': 10363,\n",
       " 'gave': 9763,\n",
       " 'tood': 23396,\n",
       " 'darned': 6198,\n",
       " 'dark': 6191,\n",
       " 'noisy': 15492,\n",
       " 'still': 21875,\n",
       " 'pricey': 17716,\n",
       " 'overpriced': 16207,\n",
       " 'owners': 16259,\n",
       " 'bagel': 2058,\n",
       " 'east': 7586,\n",
       " 'certainly': 4154,\n",
       " 'charge': 4262,\n",
       " 'double': 7234,\n",
       " 'thanks': 23014,\n",
       " 'search': 20065,\n",
       " 'continues': 5406,\n",
       " 'keep': 12634,\n",
       " 'shipped': 20434,\n",
       " 'straight': 21961,\n",
       " 'nj': 15467,\n",
       " 'door': 7202,\n",
       " 'west': 25050,\n",
       " 'hayde': 10769,\n",
       " 'small': 20956,\n",
       " 'location': 13496,\n",
       " 'does': 7120,\n",
       " 'respect': 18972,\n",
       " 'recognize': 18512,\n",
       " 'vienna': 24619,\n",
       " 'beef': 2404,\n",
       " 'italian': 12189,\n",
       " 'serves': 20243,\n",
       " 'see': 20118,\n",
       " 'defalcos': 6389,\n",
       " 'scottsdale': 19974,\n",
       " 'rd': 18384,\n",
       " 'bianca': 2601,\n",
       " 'server': 20239,\n",
       " 'check': 4312,\n",
       " 'bite': 2700,\n",
       " 'joe': 12412,\n",
       " 'makes': 13876,\n",
       " 'literally': 13430,\n",
       " 'feel': 8692,\n",
       " 'stepped': 21840,\n",
       " 'into': 12045,\n",
       " 'american': 1193,\n",
       " 'farmhouse': 8596,\n",
       " 'situated': 20749,\n",
       " 'somewhere': 21219,\n",
       " '1940': 129,\n",
       " 'employees': 7842,\n",
       " 'fully': 9568,\n",
       " 'compitent': 5133,\n",
       " 'abilities': 546,\n",
       " 'work': 25392,\n",
       " 'root': 19391,\n",
       " 'beer': 2414,\n",
       " 'tasty': 22747,\n",
       " 'leaves': 13135,\n",
       " 'aftertaste': 896,\n",
       " 'serve': 20237,\n",
       " 'barq': 2226,\n",
       " 'soda': 21141,\n",
       " 'tap': 22683,\n",
       " 'impossible': 11625,\n",
       " 'anywhere': 1391,\n",
       " 'southwest': 21327,\n",
       " 'outdoor': 16098,\n",
       " 'dining': 6826,\n",
       " 'benches': 2496,\n",
       " 'picnic': 17014,\n",
       " 'setting': 20262,\n",
       " 'indoor': 11758,\n",
       " 'features': 8675,\n",
       " 'vintage': 24658,\n",
       " 'john': 12421,\n",
       " 'deere': 6385,\n",
       " 'tractor': 23533,\n",
       " 'piece': 17024,\n",
       " 'dishes': 6942,\n",
       " 'scoop': 19942,\n",
       " 'might': 14532,\n",
       " 'want': 24852,\n",
       " 'opt': 15959,\n",
       " 'mac': 13754,\n",
       " 'rocks': 19328,\n",
       " 'cornbread': 5536,\n",
       " 'lemon': 13187,\n",
       " 'cake': 3693,\n",
       " 'desert': 6602,\n",
       " 'brisket': 3297,\n",
       " 'south': 21319,\n",
       " 'priced': 17710,\n",
       " 'stop': 21934,\n",
       " 'wife': 25199,\n",
       " 'spent': 21449,\n",
       " 'poking': 17330,\n",
       " 'around': 1599,\n",
       " 'ended': 7889,\n",
       " 'ordering': 15995,\n",
       " 'cat': 4037,\n",
       " 'mapper': 13988,\n",
       " 'recliner': 18507,\n",
       " 'throne': 23161,\n",
       " 'hunt': 11418,\n",
       " 'anymore': 1383,\n",
       " 'rarely': 18335,\n",
       " 'coupon': 5655,\n",
       " 'headed': 10794,\n",
       " 'surprised': 22374,\n",
       " 'lot': 13609,\n",
       " 'filing': 8819,\n",
       " 'juicy': 12500,\n",
       " 'toppings': 23425,\n",
       " 'fries': 9450,\n",
       " 'above': 556,\n",
       " 'stand': 21717,\n",
       " 'five': 8929,\n",
       " 'guys': 10497,\n",
       " 'big': 2623,\n",
       " 'fan': 8560,\n",
       " 'those': 23126,\n",
       " 'agree': 922,\n",
       " 'friendly': 9446,\n",
       " 'lots': 13612,\n",
       " 'parking': 16488,\n",
       " 'until': 24250,\n",
       " 'potbelly': 17506,\n",
       " 'opens': 15933,\n",
       " 'couple': 5651,\n",
       " 'weeks': 25008,\n",
       " 'packed': 16296,\n",
       " 'sidenote': 20616,\n",
       " 'pass': 16535,\n",
       " 'habit': 10520,\n",
       " 'review': 19088,\n",
       " 'else': 7780,\n",
       " 'being': 2451,\n",
       " 'said': 19598,\n",
       " 'stopped': 21935,\n",
       " 'old': 15831,\n",
       " 'friend': 9442,\n",
       " 'booth': 3010,\n",
       " 'inside': 11901,\n",
       " 'quieter': 18170,\n",
       " 'decor': 6346,\n",
       " 'true': 23738,\n",
       " 'tom': 23358,\n",
       " 'heritage': 10945,\n",
       " 'modernized': 14774,\n",
       " 'historical': 11059,\n",
       " 'pics': 17018,\n",
       " 'wall': 24825,\n",
       " 'governor': 10142,\n",
       " 'attractive': 1838,\n",
       " 'gleaming': 9967,\n",
       " 'golden': 10056,\n",
       " 'state': 21768,\n",
       " 'seal': 20053,\n",
       " 'doorway': 7208,\n",
       " 'specialties': 21410,\n",
       " 'water': 24923,\n",
       " 'info': 11812,\n",
       " '1929': 127,\n",
       " 'cuban': 5962,\n",
       " 'instead': 11941,\n",
       " 'greens': 10267,\n",
       " 'tiny': 23280,\n",
       " 'atoes': 1786,\n",
       " 'sweet': 22471,\n",
       " 'tavern': 22770,\n",
       " 'somebody': 21205,\n",
       " 'mother': 14940,\n",
       " 'abuelo': 581,\n",
       " 'using': 24370,\n",
       " 'ingredients': 11835,\n",
       " 'yellowknife': 25586,\n",
       " 'yukon': 25676,\n",
       " 'territory': 22955,\n",
       " 'test': 22970,\n",
       " 'ambiance': 1176,\n",
       " 'cleanliness': 4747,\n",
       " 'pho': 16940,\n",
       " 'generous': 9824,\n",
       " 'portion': 17444,\n",
       " 'compared': 5099,\n",
       " 'vietnamese': 24622,\n",
       " 'places': 17170,\n",
       " 'owner': 16258,\n",
       " 'entertaining': 7968,\n",
       " 'spring': 21585,\n",
       " 'smaller': 20957,\n",
       " 'anything': 1386,\n",
       " 'valley': 24438,\n",
       " 'occasionally': 15721,\n",
       " 'basic': 2264,\n",
       " 'particular': 16518,\n",
       " 'macdonald': 13768,\n",
       " 'watch': 24916,\n",
       " 'some': 21204,\n",
       " 'interesting': 12005,\n",
       " 'regulars': 18686,\n",
       " 'among': 1218,\n",
       " 'others': 16078,\n",
       " 'always': 1148,\n",
       " 'means': 14270,\n",
       " 'exciting': 8274,\n",
       " 'venue': 24539,\n",
       " 'am': 1149,\n",
       " 'purpose': 18062,\n",
       " 'mood': 14873,\n",
       " 'traveling': 23625,\n",
       " 'often': 15794,\n",
       " 'usually': 24375,\n",
       " 'count': 5631,\n",
       " 'standard': 21718,\n",
       " 'fare': 8586,\n",
       " 'catching': 4049,\n",
       " 'plane': 17181,\n",
       " 'morning': 14907,\n",
       " 'greek': 10258,\n",
       " 'prefer': 17610,\n",
       " 'thing': 23089,\n",
       " 'set': 20259,\n",
       " 'takeoff': 22619,\n",
       " 'mimosas': 14582,\n",
       " 'agreed': 924,\n",
       " 'greatest': 10252,\n",
       " 'expects': 8341,\n",
       " '5star': 388,\n",
       " 'cafe': 3676,\n",
       " 'mimosa': 14581,\n",
       " 'chill': 4452,\n",
       " 'fun': 9575,\n",
       " 'drink': 7359,\n",
       " 'squeezed': 21638,\n",
       " 'oj': 15821,\n",
       " 'trip': 23697,\n",
       " 'bought': 3073,\n",
       " 'locally': 13491,\n",
       " 'prepared': 17635,\n",
       " 'omelettes': 15857,\n",
       " 'mushroom': 15080,\n",
       " 'refuse': 18648,\n",
       " 'burgers': 3511,\n",
       " 'sandwiches': 19716,\n",
       " 'loki': 13532,\n",
       " 'vegetarian': 24496,\n",
       " 'disappointed': 6877,\n",
       " 'ate': 1766,\n",
       " 'through': 23164,\n",
       " 'soft': 21152,\n",
       " 'mediocre': 14317,\n",
       " 'seemed': 20130,\n",
       " 'scattered': 19878,\n",
       " 'unfocused': 24103,\n",
       " 'took': 23398,\n",
       " 'hamburger': 10581,\n",
       " 'outside': 16128,\n",
       " 'overcooked': 16157,\n",
       " 'dry': 7423,\n",
       " 'soggy': 21167,\n",
       " 'duck': 7446,\n",
       " 'fat': 8620,\n",
       " 'potato': 17502,\n",
       " 'fritters': 9475,\n",
       " 'coal': 4878,\n",
       " 'oven': 16140,\n",
       " 'tell': 22866,\n",
       " 'smoke': 21003,\n",
       " 'hanging': 10631,\n",
       " 'iron': 12144,\n",
       " 'skillet': 20798,\n",
       " 'pan': 16390,\n",
       " 'adds': 753,\n",
       " 'zero': 25737,\n",
       " 'hope': 11217,\n",
       " 'omg': 15864,\n",
       " 'rave': 18364,\n",
       " 'disgusting': 6939,\n",
       " 'family': 8555,\n",
       " 'each': 7551,\n",
       " 'suppose': 22344,\n",
       " 'popular': 17414,\n",
       " 'unfortunately': 24110,\n",
       " 'waiters': 24795,\n",
       " 'cursing': 6045,\n",
       " 'loud': 13618,\n",
       " 'icky': 11493,\n",
       " 'smell': 20981,\n",
       " 'oil': 15814,\n",
       " 'ick': 11492,\n",
       " 'ao': 1394,\n",
       " 'sen': 20171,\n",
       " 'least': 13130,\n",
       " '2004': 179,\n",
       " 'known': 12826,\n",
       " 'nun': 15634,\n",
       " 'moved': 14981,\n",
       " 'az': 1989,\n",
       " 'drawn': 7319,\n",
       " 'neon': 15324,\n",
       " 'light': 13304,\n",
       " 'funny': 9600,\n",
       " 'beginning': 2435,\n",
       " 'affair': 862,\n",
       " 'since': 20691,\n",
       " 'anyone': 1384,\n",
       " 'myself': 15121,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the vocabulary that was generated, containing 16,825 unique words.\n",
    "#   'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix.\n",
    "# /scrub/\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "# /scrub/\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, CountVectorizer converts all text to lowercase before generating features. Otherwise, \"Pizza\" at the start of a sentence becomes a different feature from \"pizza\" in the middle of a sentence.\n",
    "\n",
    "On the other hand, you would want different features corresponding to \"Apple\" the company and \"apple\" the fruit, so this step does discard some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't convert to lowercase.\n",
    "# /scrub/\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (1 min., post right away)**\n",
    "\n",
    "- What is the vocabulary size for CountVectorizer on this dataset with `lowercase=False`? Is this size greater or smaller than the size with `lowercase=True`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 32420)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## /scrub/\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "It is 32,420, whereas with `lowercase=True` it was 25,797. The vocabulary size is greater with `lowercase=False` because it treats words that are spelled the same but with different capitalization as distinct words, where setting `lowercase=True` causes them to be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrices using default options for CountVectorizer.\n",
    "# /scrub/\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes to predict the star rating.\n",
    "# /scrub/\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8212\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy.\n",
    "# /scrub/\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1716\n",
       "0     784\n",
       "Name: positive_rating, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check label balance\n",
    "# /scrub/\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first create an array with the same shape as y\n",
    "# then fill it in with the most common value -- numpy \"broadcasts\" the sum over the whole array\n",
    "# /scrub/\n",
    "most_common_value = y_train.value_counts().idxmax()\n",
    "null_pred = np.zeros(y_test.shape) + most_common_value\n",
    "null_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6864"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then compare predicting the mean every time to the true values\n",
    "# /scrub/\n",
    "null_accuracy = metrics.accuracy_score(null_pred, y_test)\n",
    "null_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimator predicted with ~82% accuracy, which is an improvement over this baseline 69% accuracy (always predicting a positive rating).\n",
    "\n",
    "Let's look more into how the vectorizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts a vectorizer and calculates the accuracy.\n",
    "# /scrub/\n",
    "def vectorizer_test(vect):\n",
    "    vect.fit(X_train)\n",
    "    X_train_dtm = vect.transform(X_train)\n",
    "    print(('Features: ', X_train_dtm.shape[1]))\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print(('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 10000)\n",
      "('Accuracy: ', 0.8172)\n"
     ]
    }
   ],
   "source": [
    "# min_df=2 says to ignore words that occur less than twice ('df' means \"document frequency\").\n",
    "# /scrub/\n",
    "vect = CountVectorizer(min_df=2, max_features=10000)\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look next at other ways of preprocessing text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ngrams'></a>\n",
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 339112)\n",
      "('Accuracy: ', 0.792)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams.\n",
    "# /scrub/\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $n$ gets larger, the number of *unique* n-grams increases greatly, adding pure noise to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zucchini the', 'zucchini veal', 'zucchini very', 'zucchini was', 'zucchini we', 'zucchini with', 'zuccini', 'zuccini italian', 'zuccini so', 'zuch', 'zuch and', 'zuchinni', 'zuchinni peppers', 'zuchinni the', 'zuchinni wtf', 'zuma', 'zuma and', 'zuma because', 'zuma roka', 'zuma since', 'zumba', 'zumba class', 'zumba or', 'zumba the', 'zumba yoga', 'zupa', 'zupa flavors', 'zur', 'zur kate', 'zuzu', 'zuzu has', 'zuzu in', 'zuzu is', 'zuzu the', 'zuzu was', 'zuzus', 'zuzus room', 'zweigel', 'zweigel wine', 'zwiebel', 'zwiebel kräuter', 'zy', 'zy world', 'zzed', 'zzed in', 'zzzzzzzzzzzzzzzzz', 'école', 'école lenôtre', 'òc', 'òc châm']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "# /scrub/\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "\n",
    "### Stop-Word Removal\n",
    "\n",
    "- **What:** This process is used to remove common words that will likely appear in any text.\n",
    "- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n",
    "\n",
    "**What are stop words?**\n",
    "Stop words are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document. Since \"a\" appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "# /scrub/\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {`english`}, list, or None (default)\n",
    "- If `english`, a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. (If `max_df` = 0.7, then if > 70% of documents contain a word it will not be included in the feature set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'how', 'find', 'everything', 'couldnt', 'wherever', 'which', 'perhaps', 'should', 'forty', 'very', 'ever', 'over', 'thereby', 'whether', 'all', 'me', 'nobody', 'has', 'myself', 'serious', 'see', 'that', 'were', 'thus', 'whole', 'beyond', 'into', 'sincere', 'she', 'herein', 'sometimes', 'more', 'would', 'be', 'describe', 'front', 'often', 'to', 'because', 'cry', 'someone', 'enough', 'anyway', 'latter', 'due', 'before', 'sixty', 'at', 'formerly', 'beside', 'towards', 'last', 'here', 'each', 'an', 'from', 'again', 'too', 'whatever', 'until', 'further', 'no', 'toward', 'inc', 'ours', 'thin', 'may', 'via', 'whoever', 'anyone', 'being', 'other', 'mostly', 'mine', 'show', 'elsewhere', 'its', 'least', 'below', 'always', 'by', 'moreover', 'go', 'within', 'still', 'anywhere', 'their', 'interest', 're', 'meanwhile', 'for', 'first', 'he', 'wherein', 'hundred', 'eight', 'system', 'one', 'have', 'six', 'becomes', 'yet', 'off', 'without', 'will', 'they', 'latterly', 'might', 'bill', 'hereby', 'in', 'un', 'less', 'many', 'and', 'through', 'cannot', 'done', 'her', 'afterwards', 'onto', 'down', 'anything', 'except', 'also', 'hers', 'the', 'as', 'only', 'seeming', 'once', 'must', 'why', 'either', 'whom', 'almost', 'could', 'against', 'although', 'after', 'mill', 'else', 'hence', 'yourselves', 'during', 'there', 'most', 'among', 'eleven', 'whereby', 'upon', 'those', 'cant', 'whereas', 'neither', 'do', 'yours', 'back', 'nothing', 'five', 'been', 'twelve', 'across', 'give', 'or', 'when', 'hereupon', 'his', 'next', 'whither', 'whereupon', 'these', 'seem', 'full', 'behind', 'rather', 'if', 'thick', 'himself', 'three', 'side', 'empty', 'seems', 'becoming', 'thereafter', 'ourselves', 'fifteen', 'whereafter', 'made', 'keep', 'became', 'a', 'under', 'per', 'bottom', 'was', 'none', 'noone', 'whenever', 'though', 'everyone', 'already', 'any', 'while', 'nevertheless', 'fill', 'whence', 'nor', 'twenty', 'therefore', 'some', 'part', 'nine', 'we', 'never', 'out', 'so', 'alone', 'third', 'throughout', 'it', 'co', 'take', 'thru', 'you', 'somewhere', 'thereupon', 'between', 'can', 'two', 'few', 'otherwise', 'every', 'fire', 'another', 'hereafter', 'are', 'get', 'had', 'fifty', 'not', 'of', 'indeed', 'yourself', 'with', 'your', 'something', 'own', 'even', 'up', 'hasnt', 'former', 'whose', 'thence', 'move', 'everywhere', 'anyhow', 'name', 'nowhere', 'ie', 'put', 'themselves', 'our', 'four', 'such', 'where', 'both', 'now', 'am', 'along', 'but', 'what', 'con', 'top', 'him', 'is', 'amount', 'than', 'then', 'i', 'us', 'well', 'namely', 'above', 'become', 'ltd', 'my', 'about', 'who', 'call', 'herself', 'this', 'ten', 'several', 'de', 'same', 'somehow', 'etc', 'besides', 'itself', 'sometime', 'seemed', 'others', 'however', 'found', 'on', 'amongst', 'around', 'much', 'them', 'please', 'beforehand', 'therein', 'detail', 'together', 'eg', 'since', 'amoungst'})\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer stop words for English\n",
    "# /scrub/\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "print((vect.get_stop_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_features`: int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100)\n",
      "('Accuracy: ', 0.7452)\n"
     ]
    }
   ],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "# /scrub/\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing', 'area', 'atmosphere', 'awesome', 'bad', 'bar', 'beer', 'best', 'better', 'big', 'bit', 'bread', 'burger', 'came', 'cheese', 'chicken', 'coffee', 'come', 'day', 'definitely', 'delicious', 'did', 'didn', 'dinner', 'don', 'drink', 'drinks', 'eat', 'experience', 'favorite', 'feel', 'food', 'fresh', 'friendly', 'fries', 'going', 'good', 'got', 'great', 'happy', 'home', 'hot', 'hour', 'just', 'know', 'like', 'little', 'll', 'location', 'long', 'looking', 'lot', 'love', 'lunch', 'make', 'meal', 'menu', 'minutes', 'new', 'nice', 'night', 'order', 'ordered', 'people', 'phoenix', 'pizza', 'place', 'pretty', 'price', 'prices', 'really', 'restaurant', 'right', 'said', 'salad', 'sandwich', 'sauce', 'say', 'service', 'small', 'staff', 'stars', 'sure', 'sweet', 'table', 'tasty', 'thing', 'things', 'think', 'time', 'times', 'try', 've', 'wait', 'want', 'wasn', 'way', 'went', 'wine', 'worth']\n"
     ]
    }
   ],
   "source": [
    "# All 100 features\n",
    "# /scrub/\n",
    "print((vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In our case, using about 26,000 unigram features rather than 339,112 bigram features gave us a much smaller, simpler, and easier-to-think-about model and also resulted in higher accuracy. Our model and our dataset size were not sufficient to pick out the signal within the noise that the bigrams added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-grams and 2-grams, up to 10K features:\n",
      "('Features: ', 10000)\n",
      "('Accuracy: ', 0.8316)\n",
      "\n",
      "1-grams only, up to 10K features:\n",
      "('Features: ', 10000)\n",
      "('Accuracy: ', 0.818)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features.\n",
    "# /scrub/\n",
    "\n",
    "print('1-grams and 2-grams, up to 10K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=10000)\n",
    "vectorizer_test(vect)\n",
    "\n",
    "print()\n",
    "print('1-grams only, up to 10K features:')\n",
    "vect = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 95504)\n",
      "('Accuracy: ', 0.8388)\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "# /scrub/\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding bigrams does improve performance modestly if we prune down the set of features that we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (2 mins., post right away)**\n",
    "\n",
    "How does each of the following changes to the feature representation used affect the bias and variance of a resulting model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Increases bias, decreases variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using both unigrams and bigrams instead of just unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Decreases bias, increases variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Maybe increases bias very slightly, decreases variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decreasing `max_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/scrub/\n",
    "\n",
    "Increases bias, decreases variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob'></a>\n",
    "## Introduction to TextBlob\n",
    "\n",
    "You should already have downloaded TextBlob, a Python library used to explore common NLP tasks. If you haven’t, please return to [this step](#textblob_install) for instructions on how to do so. We’ll be using this to organize our corpus for analysis.\n",
    "\n",
    "As mentioned earlier, you can read more on the [TextBlob website](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# Print the first review.\n",
    "# /scrub/\n",
    "print((yelp.text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a TextBlob object.\n",
    "# /scrub/\n",
    "review = TextBlob(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the words.\n",
    "# /scrub/\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the sentences.\n",
    "# /scrub/\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some string methods are available.\n",
    "# /scrub/\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stem'></a>\n",
    "## Stemming and Lemmatization\n",
    "Interesting read: [Stemming and Lemmatization by Stanford NLP Lab](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form.\n",
    "- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n",
    "- **Notes:**\n",
    "    - Stemming uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Snowball stemming algorithm (a.k.a. Porter2)](http://snowball.tartarus.org/algorithms/english/stemmer.html) is  an improved version of the original [Porter's stemming algorithm](http://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf) by Martin Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer.\n",
    "# /scrub/\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semi-busi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'i', 've', 'ever', 'had', 'i', \"'m\", 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', '2', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'i', 've', 'ever', 'had', 'anyway', 'i', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Stem each word.\n",
    "# /scrub/\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n",
    "\n",
    "- **What:** Lemmatization derives the canonical form (\"lemma\") of a word.\n",
    "- **Why:** It can be better than stemming.\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'The', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'be', 'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'look', 'excellent', 'I', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'It', 'come', 'with', '2', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'It', 'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize assume every word is a verb.\n",
    "# /scrub/\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"was\" lemmatized to \"be\" and \"arrived\" lemmatized to \"arrive\".\n",
    "\n",
    "Without `post='v'`, the `lemmatize` method used here assumes that each word is a noun, and it does not do much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Lemmatization and Stemming Examples**\n",
    "\n",
    "|Word|Lemmatization|Stemming|\n",
    "|-----|-------------|---------|\n",
    "|shouted|shout|shout|\n",
    "|best | good|best|\n",
    "|better | good|better|\n",
    "|good | good|good|\n",
    "|hidden | hide|hidden|\n",
    "|computing |compute| comput|\n",
    "|computed |compute| comput|\n",
    "|wipes |wipe| wip|\n",
    "|wiped |wipe| wip|\n",
    "|wiping |wipe| wip|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas.\n",
    "# /scrub/\n",
    "def split_into_lemmas(text):\n",
    "    text = str(text).lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 26735)\n",
      "('Accuracy: ', 0.8168)\n"
     ]
    }
   ],
   "source": [
    "# Use split_into_lemmas as the feature extraction function (Warning: SLOW!).\n",
    "# /scrub/\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas, decode_error='replace')\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zillion', 'zin', 'zinburger', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zip-loc', 'zipcar', 'zipper', 'zipps', 'zippy', 'ziti', 'zoe', 'zoey', 'zoftik', 'zombie', 'zombie-like', 'zone', 'zoned', 'zoners', 'zoning', 'zoo', 'zoom', 'zoomed', 'zoyo', 'zpizza', 'zucca', 'zucchini', 'zucchini/mushroom', 'zuccini', 'zuch', 'zuchinni', 'zuma', 'zuma-roka', 'zumba', 'zupa', 'zur', 'zuzu', 'zuzus', 'zweigel', 'zwiebel-kräuter', 'zzed', 'zzzzzzzzzzzzzzzzz', '¡cash', '¡excellent', '¡muy', 'école', 'òc']\n"
     ]
    }
   ],
   "source": [
    "# Last 50 features\n",
    "# /scrub/\n",
    "print((vect.get_feature_names()[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "## Term Frequency–Inverse Document Frequency (TF–IDF)\n",
    "     \n",
    "- **What:** Term frequency–inverse document frequency (TF–IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n",
    "- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n",
    "- **Notes:** It's used for search-engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency\n",
    "vect = CountVectorizer()\n",
    "vect.fit(simple_train)\n",
    "tf = pd.DataFrame(vect.transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "vect.fit(simple_train)\n",
    "df = vect.transform(simple_train).toarray().sum(axis=0) # can't use axis='index' in NumPy\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term frequency–inverse document frequency (simple version)\n",
    "# /scrub/\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the TF–IDF value, the more \"important\" the word is to that specific document. Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. TF–IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer -- uses a slightly different implementation of the same basic idea\n",
    "# /scrub/\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(simple_train)\n",
    "pd.DataFrame(vect.transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Details of the sklearn implementation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) -- the idf term has a log and some smoothing, and each row is normalized to have unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 4542)\n",
      "('Accuracy: ', 0.774)\n"
     ]
    }
   ],
   "source": [
    "# Try it on our data\n",
    "# /scrub/\n",
    "vect = TfidfVectorizer(min_df=10, stop_words='english')\n",
    "vectorizer_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_tfidf'></a>\n",
    "## Using TF–IDF to Summarize a Yelp Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF tries to pick out the most distinctive words in a given document relative to the overall corpus. Thus, we would expect that using the words with the highest TF-IDF scores for a given document would give us a better sense of what that document is about than using an equal number of random words from that document. Let's give this idea a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document-term matrix using TF–IDF.\n",
    "# /scrub/\n",
    "vect = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit transform Yelp data.\n",
    "# /scrub/\n",
    "vect.fit(yelp.text)\n",
    "dtm = vect.transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # Choose a random review that is at least 300 characters.\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # Create a dictionary of words and their TF–IDF scores.\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # Print words with the top five TF–IDF scores.\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # Print five random words.\n",
    "    print(('\\n' + 'RANDOM WORDS:'))\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # Print the review.\n",
    "    print(('\\n' + review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "simply\n",
      "facilities\n",
      "golf\n",
      "spa\n",
      "resort\n",
      "\n",
      "RANDOM WORDS:\n",
      "happen\n",
      "20\n",
      "valley\n",
      "little\n",
      "experience\n",
      "\n",
      "My wife and I decided to take a little vaca to celebrate our 13th anniversary and also was the first time that we were traveling with our 5-month old, so you can imagine we were a little apprehensive. \n",
      "\n",
      "The resort is absolutely top-notch, the rooms are very spacious, the staff is very courteous, and the facilities are simply superb.  Our room overlooked the mountain, and in the morning, it was blissful to sit outside and sip on our cups of joe. \n",
      "\n",
      "The rooms have a microwave and a small fridge, something that was invaluable to us to store milk and warm it up for our little one.  The spa is simply one of the best in the valley, and even if you don't stay here, I would recommend the spa to anyone.  The resort is close to at least 20 golf courses, something that might be important to other golf nuts like me. \n",
      "\n",
      "So then why not 5 stars? Simply because they would not extend the check-out time for us by even an hour.  The check-out is 11 am and our flight was delayed to 8 pm, so when I requested, I was told VERY NICELY, but firmly that it wasn't going to happen.  I've traveled some, and have NEVER been turned down for a late check-out request, ever. \n",
      "\n",
      "Still, a great place to stay, the facilities are top-notch, the staff is very nice, and we had a wonderful time.  Would recommend this to anyone wanting a relaxing, fun experience in the Valley of the Sun.  Cheers.\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "## Sentiment Analysis\n",
    "\n",
    "Understanding how positive or negative a review is. There are many ways in practice to compute a sentiment value. For example:\n",
    "\n",
    "- Have a list of \"positive\" words and a list of \"negative\" words and count how many occur in a document. \n",
    "- Train a classifier given many examples of \"positive\" documents and \"negative\" documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "# /scrub/\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity.\n",
    "# /scrub/\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "# /scrub/\n",
    "yelp.loc[:, 'sentiment'] = yelp.loc[:, 'text'].apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a754332e8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "# /scrub/\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most positive sentiment\n",
    "# /scrub/\n",
    "yelp.loc[yelp.loc[:, 'sentiment'] == 1, 'text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "9924                                    Horrible service.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews with most negative sentiment\n",
    "# /scrub/\n",
    "yelp.loc[yelp.loc[:, 'sentiment'] == -1, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390     RIP AZ Coffee Connection.  :(  I stopped by tw...\n",
       "1287    Obsessed. Like, I've-got-the-Twangy-Tart-withd...\n",
       "3075                       Unfortunately Out of Business.\n",
       "3516    Cashew brittle, almond brittle, bacon brittle!...\n",
       "6726    Brown bag chicken sammich, mac n cheese, fried...\n",
       "9809    I have to tell you....\\n\\nI had their Jerk Chi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "# /scrub/\n",
    "yelp.loc[(yelp.loc[:, 'stars'] == 5) & (yelp.loc[:, 'sentiment'] < -0.3), 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1781    If you like the stuck up Scottsdale vibe this ...\n",
       "6222    My mother always told me, if I didn't have any...\n",
       "8833    The owner has changed hands & this place isn't...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "# /scrub/\n",
    "yelp.loc[(yelp.loc[:, 'stars'] == 1) & (yelp.loc[:, 'sentiment'] > 0.7), 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add_feat'></a>\n",
    "## Bonus: Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will add additional features to our `CountVectorizer()`-generated feature set to hopefully improve our model.\n",
    "\n",
    "To make the best models, you will want to supplement the auto-generated features with new features you think might be important. After all, `CountVectorizer()` typically lowercases text and removes all associations between words. Or, you may have metadata to add in addition to just the text.\n",
    "\n",
    "> Remember: Although you may have hundreds of thousands of features, each data point is extremely sparse. So, if you add in a new feature, e.g., one that detects if the text is all capital letters, this new feature can still have a huge effect on the model outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp.loc[:, feature_cols]\n",
    "y = yelp.loc[:, 'positive_rating']\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 25797)\n",
      "(2500, 25797)\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer with text column only.\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train.text)\n",
    "X_train_dtm = vect.transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of other four feature columns\n",
    "X_train.drop('text', axis='columns').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix.\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis='columns').astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 25801)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine sparse matrices.\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 25801)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat for testing set.\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis='columns').astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gGandenberger/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7976\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with text column only.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8016\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with all features.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more_textblob'></a>\n",
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For some reason this code does not work the first time I try to run it.\n",
    "# Spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "\n",
    "## Appendix: Intro to Naive Bayes and Text Classification\n",
    "\n",
    "Naive Bayes is a very popular classifier because it has minimal storage requirements, is fast, can be tuned easily with more data, and has found very useful applications in text classificaton. Paul Graham originally proposed using Naive Bayes to detect spam in his [Plan for Spam](http://www.paulgraham.com/spam.html).\n",
    "\n",
    "Earlier we experimented with text classification using a Naive Bayes model. What exactly are Naive Bayes classifiers? \n",
    "\n",
    "**What is Bayes?**  \n",
    "Bayes, or Bayes' Theorem, is a way to update a probability distribution given some new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equation for Bayes.  \n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n",
    "- **$P(A)$** : Probability of `Event A` occurring.\n",
    "- **$P(B)$** : Probability of `Event B` occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Applying Naive Bayes Classification to Spam Filtering\n",
    "\n",
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.** (\"Ham\" just means not spam. It can include emails that look like spam but that you opt into!)\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are conditionally independent, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "Note that each conditional probability in the numerator is easily calculated directly from the training data!\n",
    "\n",
    "So, we can calculate all of the values in the numerator by examining a corpus of spam email:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of ham email:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the higher probability, and so we predict that the email is spam.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The \"naive\" assumption of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The normalization constant (the denominator) can be ignored since it's the same for all classes.\n",
    "- The prior probability is much less relevant once you have a lot of features.\n",
    "\n",
    "### Comparing Naive Bayes With Other Models\n",
    "\n",
    "Advantages of Naive Bayes:\n",
    "\n",
    "- Model training and prediction are very fast.\n",
    "- It's somewhat interpretable.\n",
    "- No tuning is required.\n",
    "- Features don't need scaling.\n",
    "- It's insensitive to irrelevant features (with enough observations).\n",
    "- It performs better than logistic regression when the training set is very small.\n",
    "\n",
    "Disadvantages of Naive Bayes:\n",
    "\n",
    "- If \"spam\" is dependent on non-independent combinations of individual words, it may not work well.\n",
    "- Predicted probabilities are not well calibrated.\n",
    "- Correlated features can be problematic (due to the independence assumption).\n",
    "- It can't handle negative features (with Multinomial Naive Bayes).\n",
    "- It has a higher \"asymptotic error\" than logistic regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Summary\n",
    "\n",
    "- NLP techniques allow us to do machine learning with text.\n",
    "- High-level NLP tasks include part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "- Common steps for preprocessing text include splitting into words (\"tokenizing\"), discarding punctuation, converting to lowercase, and stemming/lemmatizing.\n",
    "- To apply machine learning to text, we need to convert documents into numeric vectors.\n",
    "- Bag-of-words representations ignore word order, while ngram representations preserve it to some extent.\n",
    "- TF-IDF is a powerful technique for turning the words in a document into a useful feature vector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
