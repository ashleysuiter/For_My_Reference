{"cells": [{"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n", "\n", "# Improving Our Linear Regression Model\n", "\n", "_Authors: Kevin Markham (Washington, D.C.), Ed Podojil (New York City)_"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "import seaborn as sns\n", "import pandas as pd\n", "\n", "plt.style.use('fivethirtyeight')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bikes = pd.read_csv('../assets/data/bikeshare_modified.csv',\n", "                    index_col='datetime',\n", "                    parse_dates=True\n", "                   )"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["## How to Select a Model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In a supervised learning paradigm, our primary goal is to make accurate predictions on new cases the model hasn't seen before. As a result, **the primary criterion we use to choose among models is performance on test data that wasn't used to train the model**.\n", "\n", "That being said, **we are often willing to trade accuracy for simplicity**: using fewer features and simpler modeling techniques makes the model easier to maintain, easier to interpret, and faster to run."]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["### Evaluation Metrics for Regression Problems\n", "\n", "There are several ways to measure the performance of a regression model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Mean Squared Error\n", "\n", "**Mean squared error (MSE)** is the average of the squared errors:\n", "\n", "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n", "\n", "**Notes:**\n", "\n", "- Squaring the errors is one way to make them positive, so that both overestimates and underestimates count against the model.\n", "- Linear regression minimizes MSE on the training set.\n", "- It is hard to look at an MSE value and know whether it's good or bad because it is in different units than the target variable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can picture MSE as the average area of a square one side of which is the difference between the the actual value and the regression line:\n", "\n", "![](../assets/images/mse.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Root Mean Squared Error\n", "\n", "**Root mean squared error (RMSE)** is the square root of the MSE:\n", "\n", "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n", "\n", "**Notes:**\n", "\n", "- RMSE is more helpful to a human than MSE because it is at least in the same units as the target variable. However, it's still a little hard to interpret -- it's kind of like average error, but not quite.\n", "- Minimizing MSE also minimizes RMSE, so linear regression also minimizes RMSE on the training set.\n", "\n", "You should probably always look at RMSE rather than MSE, because it's effectively the same thing for many purposes and is easier to grok."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Mean Absolute Error\n", "\n", "**Mean absolute error (MAE)** is the mean of the absolute value of the errors:\n", "\n", "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n", "\n", "**Notes:**\n", "\n", "- MAE is simply the average magnitude of the error, so it is easier to interpret than MSE or RMSE.\n", "- Linear regression does NOT minimize MAE on the training set.\n", "\n", "If the units of your target variable are easy to grasp (e.g. dollars), then reporting MAE is probably a good idea."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can picture MAE as simply the average distance between the prediction and the actual value:\n", "\n", "![](../assets/images/mae.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (3 mins.)** Calculate MAE, MSE, and RMSE for the values below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example true and predicted response values\n", "true = [10, 7, 5, 5]\n", "pred = [8, 6, 5, 10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Fill in these calculations.\n", "\n", "*Hints*: \n", "\n", "- Turn the lists in numpy arrays.\n", "- There is a built-in `abs` function for absolute value.\n", "- NumPy has a square root function `np.sqrt`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Starter code -- uncomment and fill in the commented-out lines\n", "true = np.array(true)\n", "# pred = \n", "n = len(true)\n", "\n", "mae = sum(abs(true - pred))/n\n", "# mse =\n", "# rmse =\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Run this cell to check your answers. (It should raise an AssertionError if you made a mistake.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import metrics\n", "np.testing.assert_almost_equal(mae, metrics.mean_absolute_error(true, pred))\n", "np.testing.assert_almost_equal(mse, metrics.mean_squared_error(true, pred))\n", "np.testing.assert_almost_equal(rmse, np.sqrt(metrics.mean_squared_error(true, pred)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Operational Differences Between (R)MSE and MAE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Choosing between (R)MSE and MAE has consequences beyond ease of interpretation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import metrics\n", "\n", "true = [10, 7, 5, 5]\n", "pred1 = [8, 6, 5, 10]  # Makes small-to-medium errors\n", "pred2 = [10, 7, 5, 13]  # Makes one big error\n", "print('Pred1 MAE:', metrics.mean_absolute_error(true, pred1))\n", "print('Pred1 MSE:', metrics.mean_squared_error(true, pred1))\n", "print()\n", "print('Pred2 MAE:', metrics.mean_absolute_error(true, pred2))\n", "print('Pred2 MSE:', metrics.mean_squared_error(true, pred2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`pred1` and `pred2` have the same MAE (average error), but `pred2` has a much larger MSE. This discrepancy between MAE and MSE comes from the fact that **MSE is more sensitive to large one-off errors than MAE**. When choosing between MAE and MSE, **think about whether or not this property is appropriate for your problem**. For instance, is an error of 4 twice as bad as an error of 2 (MAE), or is it four times as bad (MSE)?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### R-Squared and Comparing to a \"Null Model\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can use (R)MSE and MAE to compare different models, but how do you know whether any of your models are any good?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A good general strategy is to compare your model to a **null model** that just predicts the mean of the target variable every time, without using any of the features. If your model is not substantially better than the null model, then it probably is not worth using."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**R-squared** is a metric that directly addresses the question of how well your model does compared to the null model.\n", "\n", "$$R^2=1-\\frac{\\mbox{Your Model MSE}}{\\mbox{Null Model MSE}}$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In statistics, the null model MSE is called \"Mean Squared Total\" (MST)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](../assets/images/r_squared.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Notes:**\n", "\n", "- $R^2=0$ means that your model is no better than the null model.\n", "- $R^2=1$ is the maximum -- it means that your model is making perfect predictions (which is suspicious).\n", "- $R^2<0$ means that your model is worse than the null model (which can happen).\n", "- $R^2$ is often called \"percent of variance explained.\" I prefer **percent of variance captured** because \"explained\" suggests that the model gives a *causal* explanation, which is typically false."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use the sklearn .score() method to calculate test-set R-squared\n", "# for a linear regression model with features\n", "# ['temp_celsius', 'season_num', 'humidity_percent']\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Advice for Using Regression Evaluation Metrics\n", "\n", "- Use R-squared to compare your model to a \"null model\" that just guesses the mean every time. This step is often useful when you are just getting started on a regression problem.\n", "- Use RMSE or MAE to compare models using a number that's relatively interpretable. Use MSE if an occasional large error is much worse than frequent small errors of the same total magnitude, MAE otherwise.\n", "\n", "Regardless of what metric you use, performance on the **test set** is what matters in the end."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Comparing Models With Train/Test Split and RMSE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define a function that accepts a list of features and returns testing RMSE.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compare different sets of features.\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## Selecting Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Blindly throwing all of your variables into a linear regression model is not a great strategy. Let's inspect our data and come up with something smarter."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We want features that are strongly correlated with the target and not with each other."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (6 mins.)**\n", "\n", "- Create another `LinearRegression` instance that is fit using `temp_celsius` and `atemp_celsius`, and print the coefficients."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Create another `LinearRegression` instance that is fit using `atemp_celsius` only, and print the coefficients."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Why is the coefficient for `atemp_celsius` so different in the two models?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This case is an example of *collinearity*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Collinearity happens when two or more features are highly correlated with each other. It causes problems:\n", "\n", "- Coefficients become hard to interpret\n", "- Adding a variable to the model that is collinear with another variable in the model adds complexity while contributing limited new information.\n", "- Model fitting can become numerically unstable."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize correlation matrix in Seaborn using a heat map.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- `num_total_users` is of course strongly correlated with `num_casual_users` and `num_registered_users`, but those features are not usable for prediction.\n", "- It is most positively correlated with `temp_celsius` and `atemp_celsius` among usable features, but those features are largely redundant because they are strongly correlated with one another.\n", "- It is also moderately negatively correlated with `humidity_percent`. (Negative correlation is just as useful for prediction as positive correlation.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's see what happens to RMSE when we add `atemp_celsius` to a model that \n", "# already has `temp_celsius`\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Handling Categorical and Ordinal Features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Box plot of rentals, grouped by season_num.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (4 mins., in groups)**\n", "\n", "We are representing the seasons as follows:\n", "\n", "1: winter\n", "2: spring\n", "3: summer\n", "4: fall\n", "\n", "Suppose we fit a linear regression model for ridership against `season_num` only and get a coefficient 10, so that our model has this form:\n", "\n", "$\\mbox{num_total_users} = \\beta_0 + 10 * \\mbox{season_num}$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change from winter to spring?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change from spring to summer?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change from summer to fall?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Why might this consequence of using `season_num` in a linear regression model be a problem?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here's a better representation:\n", "\n", "- `is_spring`: 1 in spring, 0 otherwise\n", "- `is_summer`: 1 in summer, 0 otherwise\n", "- `is_fall`: 1 in fall, 0 otherwise\n", "\n", "Instead of having one variable with 4 levels, we have 3 binary variables.\n", "\n", "We can now create a linear regression model that has three \"degrees of freedom\" to capture the impact of the season, instead of just one:\n", "\n", "$\\mbox{num_total_users} = \\beta_0 + \\beta_1 * \\mbox{is_spring} + \\beta_2 * \\mbox{is_summer} + \\beta_3 * \\mbox{is_fall}$.\n", "\n", "This process is called \"dummy coding.\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (5 mins., in groups)**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Suppose we fit a linear regression model for ridership against our dummy variables for season only and get the following coefficients:\n", "\n", "$\\mbox{num_total_users} = \\beta_0 + 10 * \\mbox{is_spring} + 25 * \\mbox{is_summer} + 5 * \\mbox{is_fall}$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change for spring relative to winter?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change for summer relative to spring?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How do the model's predictions change for fall relative to summer?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Why is `is_winter` unnecessary?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- In fact, including `is_winter` would be harmful. Why?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Could you use `is_winter`, `is_spring`, and `is_summer` instead? Why or why not? Is there any reason to prefer one set of dummy variables over another?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Take every 1000 rows for the moment to make it easier to see what's going on\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Now dummy-code `season_num`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Drop a redundant column\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Or do it in one step:\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike `season`, our variable `weather` is at least ordinal:\n", "\n", "1: Clear, 2: Mist, 3: Light Rain or Snow 4: Heavy Rain or Snow\n", "\n", "However, there's no reason to assume that the difference between Clear and Mist has the same impact on ridership as the difference between Mist and List Rain/Snow or the difference between Light Rain/Snow and Heavy Rain/Snow. Here too our model can benefit from the additional degrees of freedom that dummy coding provides."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (3 mins.)**\n", "\n", "Use the full data set `bikes` for these exercises, rather than the reduced `bikes_sample_seasons`.\n", "\n", "- Use pandas to create dummy columns for `weather`, and drop the column that you want to use as a baseline."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How many dummy variables do you need for a categorical feature with $k$ possible values? Why?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's rerun the linear regression with dummy variables included."]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Include dummy variables for season_num in the model.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (6 mins.)**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- What does the coefficient for summer mean?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Huh? How can we get a coefficient of -46.37 in summer, the most popular season?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Would our model's predictions change if we dropped a different dummy column for season?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Use train_test_rmse to compare the performance of a model with `temp_celsius`, `season_num`, and `humidity_percent` to one that replaces `season_num` with its dummies."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["## Summary\n", "\n", "- Test a model against data that wasn't used to train it.\n", "- For a regression model, use a metric such as MAE, (R)MSE, or R-squared.\n", "- Experiment with adding and dropping features.\n", "- Favor features that are correlated with the target and uncorrelated with other features.\n", "- Experiment with different ways to represent features, such as dummy-coding for categorical and ordinal variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing Linear Regression With Other Models\n", "\n", "Advantages of linear regression:\n", "\n", "- Simple to explain.\n", "- Interpretable -- sort of.\n", "- Model training and prediction are fast.\n", "- No tuning is required (excluding regularization).\n", "- Features don't need scaling.\n", "- Can perform well with a small number of observations.\n", "- Well understood.\n", "\n", "Disadvantages of linear regression:\n", "\n", "- Presumes a linear relationship between the features and the response. (But you can model nonlinear relationships with your *original* features by including *transformations* of those features, e.g. the square of one of those features.)\n", "- Performance is (generally) not competitive with the best supervised learning methods due to high bias.\n", "- Can't automatically learn feature interactions.\n", "- Easy to overinterpret."]}], "metadata": {"kernelspec": {"display_name": "ga", "language": "python", "name": "ga"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}}, "nbformat": 4, "nbformat_minor": 2}