{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Addressing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing model complexity can decrease bias but will eventually lead to overfitting. This lesson is about what to do when your model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Getting More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting more data is the surest way to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df = pd.read_csv('../assets/data/ames_train.csv')\n",
    "# Dropping `OverallQual` because it is not always available\n",
    "ames_df = ames_df.drop(['Id', 'OverallQual'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 79)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
       "0         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
       "1         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
       "2         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
       "3         Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
       "4         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition SalePrice  \n",
       "0       0      2    2008        WD         Normal    208500  \n",
       "1       0      5    2007        WD         Normal    181500  \n",
       "2       0      9    2008        WD         Normal    223500  \n",
       "3       0      2    2006        WD        Abnorml    140000  \n",
       "4       0     12    2008        WD         Normal    250000  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to calculate training score on the whole dataset\n",
    "# and cross-validated test score.\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "def my_cv_score(estimator, X,y,n_splits=10):\n",
    "    estimator.fit(X,y)\n",
    "    train_score = estimator.score(X,y)\n",
    "    \n",
    "    kf = KFold(n_splits, shuffle=True, random_state=42)\n",
    "    test_score = cross_val_score(estimator, X,y,cv=kf).mean()\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8523830520786465, 0.733740827963025)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 5-fold cross-validation to calculate training-set and test-set scores using 200 rows of data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ames_df_num = ames_df.select_dtypes(['int64','float64']).dropna(axis='columns')\n",
    "ames_small = ames_df_num.sample(200, random_state=42)\n",
    "\n",
    "target_col = 'SalePrice'\n",
    "\n",
    "X_small = ames_small.drop(target_col, axis='columns')\n",
    "y_small = ames_small.loc[:,target_col]\n",
    "\n",
    "lr = LinearRegression()\n",
    "my_cv_score(lr, X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7788559155673488, 0.7307411610581319)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 5-fold cross-validation to calculate training-set and test-set scores using all the data\n",
    "X = ames_df_num.drop(target_col, axis='columns')\n",
    "y = ames_df_num.loc[:, target_col]\n",
    "\n",
    "my_cv_score(lr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When collecting more data is not too expensive, it should be your first resort to address overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (1 min.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How does getting more rows of data affect a given model type's bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Select Features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it is best not to use all of your features, for at least two reasons:\n",
    "\n",
    "1. Using more features makes your model harder to interpret.\n",
    "2. Removing features may improve overall predictive performance by reducing overfitting.\n",
    "\n",
    "These problems become particularly acute when you start using feature engineering to generate derived features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Adding Features in Order of Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one recipe for optimizing a model through feature selection:\n",
    "\n",
    "1. Rank features according to some measure of importance *calculated on the training set*.\n",
    "2. Add features to your model in the order given by that ranking, fitting on the training set and measuring predictive performance on the test set each time.\n",
    "3. Choose the set of features that gives the best predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is inappropriate to use the test set in your calculation of feature importance because you will not have the benefit of information about feature importance for the data the model sees when you deploy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using cross-validation, you need to calculate your measure of importance on the training set *for each fold*. With this approach, you can have different features in your model for different folds. You should think of cross-validation in this case as estimating the performance of your entire *model-generating procedure*, rather than the particular model that it produces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Using Random Forest Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will fit a random forest and use the feature importances as our measure of predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating random forest feature importances is computationally expensive, so we want to do it once for each fold, rather than once for each fold and each number of features.\n",
    "\n",
    "**Warning:** the code is about to get a little complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to calculate cross-validation scores by number of features\n",
    "def my_cv_score_by_num_features(estimator, X, y, rank_feature_func, n_splits=10):\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    for train_rows, test_rows in tqdm(kf.split(X, y), total=n_splits):\n",
    "        X_train = X.iloc[train_rows, :]\n",
    "        X_test = X.iloc[test_rows, :]\n",
    "        y_train = y.iloc[train_rows]\n",
    "        y_test = y.iloc[test_rows]\n",
    "        \n",
    "        ranked_features = rank_feature_func(X_train, y_train)\n",
    "        \n",
    "        train_scores_fold = []\n",
    "        test_scores_fold = []\n",
    "        \n",
    "        num_features_range = range(1, len(ranked_features) + 1)\n",
    "        for num_features in num_features_range:\n",
    "            feature_cols = ranked_features[:num_features]\n",
    "            X_train_k = X_train.loc[:, feature_cols]\n",
    "            X_test_k = X_test.loc[:, feature_cols]\n",
    "            \n",
    "            estimator.fit(X_train_k, y_train)\n",
    "            train_scores_fold.append(estimator.score(X_train_k, y_train))\n",
    "            test_scores_fold.append(estimator.score(X_test_k, y_test))\n",
    "        \n",
    "        train_scores.append(train_scores_fold)\n",
    "        test_scores.append(test_scores_fold)\n",
    "    \n",
    "    train_scores = np.array(train_scores).T.mean(axis=1)\n",
    "    test_scores = np.array(test_scores).T.mean(axis=1)\n",
    "    return pd.DataFrame({'num_features': num_features_range,\n",
    "                         'train': train_scores,\n",
    "                         'test': test_scores\n",
    "                        }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to rank features according to estimator feature importances\n",
    "def rank_features_by_estimator_importances(X, y, estimator):\n",
    "    estimator.fit(X, y)\n",
    "    feature_ranks_worst_first = estimator.feature_importances_.argsort()\n",
    "    feature_ranks_first_worst = feature_ranks_worst_first[::-1]\n",
    "    ranked_features = X.columns[feature_ranks_first_worst]\n",
    "    return ranked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more specialized function to rank features according to random\n",
    "# forest regressor feature importances\n",
    "from functools import partial\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(100)\n",
    "rank_features_by_rfr_importances = partial(rank_features_by_estimator_importances, estimator=rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get scores by number of features for a linear regression model\n",
    "# on our sampled Ames dataset, adding features in order of\n",
    "# decreasing random forest regressor importances.\n",
    "lr = LinearRegression()\n",
    "\n",
    "scores = my_cv_score_by_num_features(\n",
    "    estimator=lr,\n",
    "    X=X_small,\n",
    "    y=y_small,\n",
    "    rank_feature_func=rank_features_by_rfr_importances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_features</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.812490</td>\n",
       "      <td>0.748138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.829914</td>\n",
       "      <td>0.746557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.821910</td>\n",
       "      <td>0.742294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.832499</td>\n",
       "      <td>0.740074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.828098</td>\n",
       "      <td>0.738369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.838396</td>\n",
       "      <td>0.737675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.817991</td>\n",
       "      <td>0.736853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.845195</td>\n",
       "      <td>0.736362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.734513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.847289</td>\n",
       "      <td>0.734482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.733410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.854849</td>\n",
       "      <td>0.730602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.841807</td>\n",
       "      <td>0.729526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.808654</td>\n",
       "      <td>0.727587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.849227</td>\n",
       "      <td>0.727375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.853163</td>\n",
       "      <td>0.726682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.849752</td>\n",
       "      <td>0.724422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.857618</td>\n",
       "      <td>0.722752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.851918</td>\n",
       "      <td>0.722601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.720404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.850501</td>\n",
       "      <td>0.719908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.851343</td>\n",
       "      <td>0.718957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.799843</td>\n",
       "      <td>0.714978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.805432</td>\n",
       "      <td>0.714122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.788265</td>\n",
       "      <td>0.698622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.775802</td>\n",
       "      <td>0.696169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.772329</td>\n",
       "      <td>0.689066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.762894</td>\n",
       "      <td>0.688509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.745770</td>\n",
       "      <td>0.672088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.723828</td>\n",
       "      <td>0.644691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.652271</td>\n",
       "      <td>0.529365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.462419</td>\n",
       "      <td>0.406995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_features     train      test\n",
       "11            12  0.812490  0.748138\n",
       "15            16  0.829914  0.746557\n",
       "13            14  0.821910  0.742294\n",
       "16            17  0.832499  0.740074\n",
       "14            15  0.828098  0.738369\n",
       "17            18  0.838396  0.737675\n",
       "12            13  0.817991  0.736853\n",
       "20            21  0.845195  0.736362\n",
       "19            20  0.844377  0.734513\n",
       "21            22  0.847289  0.734482\n",
       "22            23  0.848315  0.733410\n",
       "29            30  0.854849  0.730602\n",
       "18            19  0.841807  0.729526\n",
       "10            11  0.808654  0.727587\n",
       "23            24  0.849227  0.727375\n",
       "28            29  0.853163  0.726682\n",
       "24            25  0.849752  0.724422\n",
       "31            32  0.857618  0.722752\n",
       "27            28  0.851918  0.722601\n",
       "30            31  0.856424  0.720404\n",
       "25            26  0.850501  0.719908\n",
       "26            27  0.851343  0.718957\n",
       "8              9  0.799843  0.714978\n",
       "9             10  0.805432  0.714122\n",
       "7              8  0.788265  0.698622\n",
       "6              7  0.775802  0.696169\n",
       "5              6  0.772329  0.689066\n",
       "4              5  0.762894  0.688509\n",
       "3              4  0.745770  0.672088\n",
       "2              3  0.723828  0.644691\n",
       "1              2  0.652271  0.529365\n",
       "0              1  0.462419  0.406995"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the scores\n",
    "scores.sort_values('test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x8a54b68630>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot scores\n",
    "scores.plot.scatter('num_features', 'test')\n",
    "#scores.plot?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_features</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.812490</td>\n",
       "      <td>0.748138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.829914</td>\n",
       "      <td>0.746557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.821910</td>\n",
       "      <td>0.742294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.832499</td>\n",
       "      <td>0.740074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.828098</td>\n",
       "      <td>0.738369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.838396</td>\n",
       "      <td>0.737675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.817991</td>\n",
       "      <td>0.736853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.845195</td>\n",
       "      <td>0.736362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.734513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.847289</td>\n",
       "      <td>0.734482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.733410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.854849</td>\n",
       "      <td>0.730602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.841807</td>\n",
       "      <td>0.729526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.808654</td>\n",
       "      <td>0.727587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.849227</td>\n",
       "      <td>0.727375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.853163</td>\n",
       "      <td>0.726682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.849752</td>\n",
       "      <td>0.724422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.857618</td>\n",
       "      <td>0.722752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.851918</td>\n",
       "      <td>0.722601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.856424</td>\n",
       "      <td>0.720404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.850501</td>\n",
       "      <td>0.719908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.851343</td>\n",
       "      <td>0.718957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.799843</td>\n",
       "      <td>0.714978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.805432</td>\n",
       "      <td>0.714122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.788265</td>\n",
       "      <td>0.698622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.775802</td>\n",
       "      <td>0.696169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.772329</td>\n",
       "      <td>0.689066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.762894</td>\n",
       "      <td>0.688509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.745770</td>\n",
       "      <td>0.672088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.723828</td>\n",
       "      <td>0.644691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.652271</td>\n",
       "      <td>0.529365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.462419</td>\n",
       "      <td>0.406995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_features     train      test\n",
       "11            12  0.812490  0.748138\n",
       "15            16  0.829914  0.746557\n",
       "13            14  0.821910  0.742294\n",
       "16            17  0.832499  0.740074\n",
       "14            15  0.828098  0.738369\n",
       "17            18  0.838396  0.737675\n",
       "12            13  0.817991  0.736853\n",
       "20            21  0.845195  0.736362\n",
       "19            20  0.844377  0.734513\n",
       "21            22  0.847289  0.734482\n",
       "22            23  0.848315  0.733410\n",
       "29            30  0.854849  0.730602\n",
       "18            19  0.841807  0.729526\n",
       "10            11  0.808654  0.727587\n",
       "23            24  0.849227  0.727375\n",
       "28            29  0.853163  0.726682\n",
       "24            25  0.849752  0.724422\n",
       "31            32  0.857618  0.722752\n",
       "27            28  0.851918  0.722601\n",
       "30            31  0.856424  0.720404\n",
       "25            26  0.850501  0.719908\n",
       "26            27  0.851343  0.718957\n",
       "8              9  0.799843  0.714978\n",
       "9             10  0.805432  0.714122\n",
       "7              8  0.788265  0.698622\n",
       "6              7  0.775802  0.696169\n",
       "5              6  0.772329  0.689066\n",
       "4              5  0.762894  0.688509\n",
       "3              4  0.745770  0.672088\n",
       "2              3  0.723828  0.644691\n",
       "1              2  0.652271  0.529365\n",
       "0              1  0.462419  0.406995"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at scores in descending order, identify\n",
    "# the number of features that worked best.\n",
    "scores.sort_values('test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See scikit-learn's [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) for some alernatives to random forest feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### Limitations of Adding Features in Order of Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result May Not Be Optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking an \"all-star team\" of features based on their individual performances can miss combinations of features that work better overall because they complement each other well.\n",
    "\n",
    "For instance, if you had temperature in Celsius and temperature in Fahrenheit in your data set and used correlation with the target as your measure of importance, this approach would use both of those features without regard for the fact that they are redundant. Using random forest feature importances addresses this problem to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Can Be Slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach requires fitting many times, so it can be quite slow, especially when you combine it with ensembling and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Approaches to Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with all of the features and gradually remove them.\n",
    "- Add/remove one feature at a time, each time testing how much the change helps or hurts. Choose the feature that helps the most at each stage, stopping when you cannot find one that helps substantially.\n",
    "- Choose the top $k$ features based on a measure of feature importance without fitting a model on each set of features.\n",
    "- Try all combinations of features. (This approach quickly becomes computationally infeasible.)\n",
    "- Try random combinations of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (12 mins., in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some randomness in the processes used in this exercise, so we will not all get the same answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How does dropping a feature typically affect a model's bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping features typically decreases bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a feature matrix `X` and a target series `y` for the Titanic dataset (located in this lesson's  `assets/data` directory). Drop \"PassengerId\", \"Name\", and \"Ticket.\" Dummy-code string columns as needed. Impute missing values using a median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('../assets/data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Garside, Miss. Ethel</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.361582</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.019697</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                  Name   Sex  \\\n",
       "count    891.000000  891.000000  891.000000                   891   891   \n",
       "unique          NaN         NaN         NaN                   891     2   \n",
       "top             NaN         NaN         NaN  Garside, Miss. Ethel  male   \n",
       "freq            NaN         NaN         NaN                     1   577   \n",
       "mean     446.000000    0.383838    2.308642                   NaN   NaN   \n",
       "std      257.353842    0.486592    0.836071                   NaN   NaN   \n",
       "min        1.000000    0.000000    1.000000                   NaN   NaN   \n",
       "25%      223.500000    0.000000    2.000000                   NaN   NaN   \n",
       "50%      446.000000    0.000000    3.000000                   NaN   NaN   \n",
       "75%      668.500000    1.000000    3.000000                   NaN   NaN   \n",
       "max      891.000000    1.000000    3.000000                   NaN   NaN   \n",
       "\n",
       "               Age       SibSp       Parch Ticket        Fare    Cabin  \\\n",
       "count   891.000000  891.000000  891.000000    891  891.000000      204   \n",
       "unique         NaN         NaN         NaN    681         NaN      147   \n",
       "top            NaN         NaN         NaN   1601         NaN  B96 B98   \n",
       "freq           NaN         NaN         NaN      7         NaN        4   \n",
       "mean     29.361582    0.523008    0.381594    NaN   32.204208      NaN   \n",
       "std      13.019697    1.102743    0.806057    NaN   49.693429      NaN   \n",
       "min       0.420000    0.000000    0.000000    NaN    0.000000      NaN   \n",
       "25%      22.000000    0.000000    0.000000    NaN    7.910400      NaN   \n",
       "50%      28.000000    0.000000    0.000000    NaN   14.454200      NaN   \n",
       "75%      35.000000    1.000000    0.000000    NaN   31.000000      NaN   \n",
       "max      80.000000    8.000000    6.000000    NaN  512.329200      NaN   \n",
       "\n",
       "       Embarked  \n",
       "count       889  \n",
       "unique        3  \n",
       "top           S  \n",
       "freq        644  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "25%         NaN  \n",
       "50%         NaN  \n",
       "75%         NaN  \n",
       "max         NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.loc[:,'Age']=titanic.loc[:,'Age'].fillna(titanic.loc[:,'Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  Embarked_Q  \\\n",
       "0         0       3  22.0      1      0   7.2500         1           0   \n",
       "1         1       1  38.0      1      0  71.2833         0           0   \n",
       "2         1       3  26.0      0      0   7.9250         0           0   \n",
       "3         1       1  35.0      1      0  53.1000         0           0   \n",
       "4         0       3  35.0      0      0   8.0500         1           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_1 = pd.get_dummies(titanic, columns=['Sex', 'Embarked'], drop_first = True)\n",
    "titanic_1 =titanic_1.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1)\n",
    "titanic_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8)\n"
     ]
    }
   ],
   "source": [
    "target_col = 'Survived'\n",
    "feature_col = titanic_1.drop(target_col, axis=1).columns\n",
    "X=titanic_1.loc[:,feature_col]\n",
    "y = titanic_1.loc[:,target_col]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your work\n",
    "assert X.shape == (891, 8)\n",
    "assert y.shape == (891,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to use random forest feature importances for the Titanic dataset. What would be wrong with using a `RandomForestRegressor` for this purpose? What should we use instead? (Hint: we use this dataset for classification.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `RandomForestRegressor` would be inappropriate for this problem because it does regression rather than classification. We should use a `RandomForestClassifier` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function analogous to `rank_features_by_rfr_importances` that uses a `RandomForestClassifier` in place of a `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(100)\n",
    "rank_features_by_rfc_importances = partial(rank_features_by_estimator_importances, estimator=rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `my_cv_score_by_num_features` to calculate training and cross-validation scores for a logistic regression model on the Titanic dataset by number of features, adding features in order of increasing `RandomForestClassifier` feature importance. Call your logistic regression estimator `logreg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  3.76s/it]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "scores = my_cv_score_by_num_features(\n",
    "    estimator=lr,\n",
    "    X=X_small,\n",
    "    y=y_small,\n",
    "    rank_feature_func=rank_features_by_rfc_importances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot the scores by number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_features</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.809075</td>\n",
       "      <td>0.764777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.827022</td>\n",
       "      <td>0.764513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.811779</td>\n",
       "      <td>0.762547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.840952</td>\n",
       "      <td>0.762155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.814401</td>\n",
       "      <td>0.759832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.805071</td>\n",
       "      <td>0.759542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.798621</td>\n",
       "      <td>0.757719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.844731</td>\n",
       "      <td>0.756740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.817597</td>\n",
       "      <td>0.756612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.847176</td>\n",
       "      <td>0.754869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.847069</td>\n",
       "      <td>0.754495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.842929</td>\n",
       "      <td>0.753443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.821955</td>\n",
       "      <td>0.751905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.833761</td>\n",
       "      <td>0.751596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.791464</td>\n",
       "      <td>0.749847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.848431</td>\n",
       "      <td>0.745019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.778911</td>\n",
       "      <td>0.739674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.849381</td>\n",
       "      <td>0.738373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.849777</td>\n",
       "      <td>0.737102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.850520</td>\n",
       "      <td>0.731902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.764811</td>\n",
       "      <td>0.731430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.850709</td>\n",
       "      <td>0.728250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.858317</td>\n",
       "      <td>0.725317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.851140</td>\n",
       "      <td>0.722715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.857334</td>\n",
       "      <td>0.722317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.855832</td>\n",
       "      <td>0.720404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.852741</td>\n",
       "      <td>0.712219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.750273</td>\n",
       "      <td>0.686533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.682674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.642543</td>\n",
       "      <td>0.602597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.526905</td>\n",
       "      <td>0.455700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.371413</td>\n",
       "      <td>0.306422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_features     train      test\n",
       "10            11  0.809075  0.764777\n",
       "15            16  0.827022  0.764513\n",
       "11            12  0.811779  0.762547\n",
       "17            18  0.840952  0.762155\n",
       "12            13  0.814401  0.759832\n",
       "9             10  0.805071  0.759542\n",
       "8              9  0.798621  0.757719\n",
       "19            20  0.844731  0.756740\n",
       "13            14  0.817597  0.756612\n",
       "21            22  0.847176  0.754869\n",
       "20            21  0.847069  0.754495\n",
       "18            19  0.842929  0.753443\n",
       "14            15  0.821955  0.751905\n",
       "16            17  0.833761  0.751596\n",
       "7              8  0.791464  0.749847\n",
       "22            23  0.848431  0.745019\n",
       "6              7  0.778911  0.739674\n",
       "23            24  0.849381  0.738373\n",
       "24            25  0.849777  0.737102\n",
       "25            26  0.850520  0.731902\n",
       "5              6  0.764811  0.731430\n",
       "26            27  0.850709  0.728250\n",
       "31            32  0.858317  0.725317\n",
       "27            28  0.851140  0.722715\n",
       "30            31  0.857334  0.722317\n",
       "29            30  0.855832  0.720404\n",
       "28            29  0.852741  0.712219\n",
       "4              5  0.750273  0.686533\n",
       "3              4  0.716981  0.682674\n",
       "2              3  0.642543  0.602597\n",
       "1              2  0.526905  0.455700\n",
       "0              1  0.371413  0.306422"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort_values('test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x8a55d6cf98>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores.plot(x='num_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify the number of features that gave the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bonus:** Calculate the training and test-set scores without using `my_cv_score_by_num_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a kind of dimensionality reduction: every time we drop a feature, we reduce the number of dimensions in the feature space by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special dimensionality reduction techniques allow us to reduce the dimensionality of the feature space in a way that **preserves more information** than simply dropping features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, **Principle Component Analysis** finds the \"most informative\" *combinations of features* before dropping some of those combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Visual explanation of PCA](http://setosa.io/ev/principal-component-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: PCA on Temp, Atemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_csv('../assets/data/bikeshare.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"atemp\" and \"temp\" lie almost on a line, with \"atemp\" departing from temp based on factors such as wind and humidity.\n",
    "\n",
    "PCA returns that line as its first principal component and departures from that line as its second principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot atemp against temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the anomalous points with atemp nearly zero but temp > 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look closer at the \"atemp\" values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filer out the anomalous rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot to confirm that we removed the anomalous points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformer to bikes data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out principal components\n",
    "# The first is a line with slope slightly greater than one.\n",
    "# The second is exactly perpendicular to that line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw principal components on top of scatterplot\n",
    "def draw_vector(v0, v1, ax, c='r'):\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0,\n",
    "                    shrinkB=0,\n",
    "                    color=c\n",
    "                   )\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bikes.plot(kind='scatter', x='temp', y='atemp', alpha=0.1, ax=ax)\n",
    "for length, vector, color in zip(pca.explained_variance_, pca.components_, ['r', 'g']):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax, c=color)\n",
    "    draw_vector(pca.mean_, pca.mean_ - v, ax=ax, c=color)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(ax.get_ylim());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: PCA on the Ames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames_small.drop('SalePrice', axis='columns')\n",
    "y = ames_small.loc[:, 'SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one feature is measured on a scale of 1-10 while another is on a scale of 1-1000, for instance, then PCA will tend to focus on capturing the feature that is on a scale of 1-1000. We want PCA to identify maximally informative combinations of features regardless of their scale, so it is generally a good idea to avoid this problem by standardizing features before applying PCA. Standardizing gives each feature a mean of 0 and a variance of 1 by substracting the feature mean and dividing by its standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply `StandardScaler` to `X`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm column means are 0 and variances are 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to scaled features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first two rows of PCA features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to interpret these features is usually a fool's errand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA has generated a set of features in order of decreasing variance. Variance tends to be associated with informativeness about the target, so we can use this ordering for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores vs. number of PCA features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occasionally things go badly wrong and we get a huge negative score;\n",
    "# clip scores at 0 so that we can see what is going on in the other cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA + Feature Selection Compared to Simple Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA followed by feature selection preserves more of the variation in the feature matrix than feature selection alone. As a result, it often allows you to decrease variance with less of an increase in bias. However, this outcome is not guaranteed.\n",
    "\n",
    "One downside of using PCA is that it typically makes your feature uninterpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (10 mins., in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the `glass` dataset from the file `glass.csv` in this lesson's `assets/data` directory into a Pandas DataFrame called `glass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new column \"household\" that has the value `1` where \"Type\" is greater than 4 and zero elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using \"household\" as your target variable, define a feature matrix `X` and target vector `y` for the `glass` dataset. (Do not use \"Type\" as a feature variable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply PCA to your feature matrix `X`, and call the result `X_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use our function `my_cv_score_by_num_features` to add principal components one at a time to a logistic regression model for the glass dataset and get the score at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clip and plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bonus:** Calculate scores without using `my_cv_score_by_num_features`, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization makes your model pay a penalty to use the features, so that it will use them only to the extent that the benefit in terms of improving fit on the training set outweighs the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of regularization as a **continuous generalization** of feature selection: it allows the model to rely on a given feature *less* than it would, without regularization without taking its use of that variable all the way to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Ridge Regression on the Mammals Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression** is linear regression with a twist: instead of minimizing mean squared error alone, it minimizes a weighted sum of the mean squared error and *the sum of the squares of the coefficients*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter $\\alpha$ controls how much weight is put on keeping the coefficients small vs. reducing MSE. **A larger value for $\\alpha$ leads to more regularization.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals = pd.read_csv('../assets/data/mammals.txt', delimiter='\\t', names=['brain', 'body'], header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the scope of the model to mammals with body weights under 200 kg\n",
    "# to make it easier to see what's going on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our feature matrix and target vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fifth-order polynomial features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted regression curve at four levels of alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression minimizes $SSE + \\alpha \\sum{\\beta_i^2}$, where $SSE$ is the sum of squared errors.\n",
    "\n",
    "Setting $\\alpha=0$ causes Ridge regression to minimize $SSE$ only, reducing it to standard linear regression.\n",
    "\n",
    "Increasing $\\alpha$ increases the penalty for large coefficients, which nudges the model toward simpler, flatter shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (1 min.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How does increase $\\alpha$ affect bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increases bias, decreases variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Ridge Regression on the Ames Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply Ridge regression to the Ames dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames_df_num.drop('SalePrice', axis='columns')\n",
    "y = ames_df.loc[:, 'SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's generally a good idea to standardize features before applying regularization; otherwise the effects of regularization will depend on feature scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ridge regression to reduce overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure training-set and test performance at different levels of alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put scores into a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase $\\alpha$, training-set performance will decrease but test-set performance will often increase, then decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores, with a vertical line at the best alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how increasing $\\alpha$ causes Ridge regression to choose smaller coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ridge coefficients against alpha \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is similar to Ridge regression, but it uses the sum of the absolute values of the coefficients as its penalty term instead of the sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One consequence of this difference is that Lasson regression will take coefficients all the way to zero, while Ridge regression will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Lasso coefficients against alpha \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso minimizes $SSE + \\alpha \\sum{|\\beta_i|}$, where $SSE$ is the sum of squared errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using squared coefficients as a penalty term as in Ridge regression is called **$L_2$ regularization**, while using absolute values of coefficients as in Lasso is called **$L_1$ regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only care about predictive accuracy, you can try both Lasso and Ridge regression and see which one works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you also prefer a model that is easier to interpret and maintain, then the fact that Lasso drops variables is a point in its favor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an **[elastic net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)** algorithm that uses both $L_1$ and $L_2$ penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (6 mins., in pairs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measure training-set and test-set performance at various levels of alpha between 0 and 5000 for Lasso regression on the Ames housing dataset features we used above. Set `max_iter=10_000` when you instantiage the Lasso class to ensure that the algorithm converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot those scores as a function of $\\alpha$, with a vertical line indicating the best value for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What value for $\\alpha$ gave the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_1$ and $L_2$ regularization can also be used with other models than linear regression. For instance, scikit-learn supports them for [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), and [XGBoost](https://xgboost.readthedocs.io/en/latest/) incorporates them into decision tree ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The surest way to address overfitting is to get more data. When getting more data is not an option, you can try feature selection, dimensionality reduction, and regularization.\n",
    "- Feature selection can yield good predictive performance, is easy to understand, and yields simpler models.\n",
    "- Dimensionality reduction, for instance with PCA, can yield better predictive performance by giving the model more information to work with at a given degree of complexity. However, it is harder to understand and typically yields uninterpretable models.\n",
    "- Regularization can be thought of as a continuous generalization of feature selection: it nudges all coefficients toward zero rather than setting some of them fully to zero without affecting the others. It can drop some variables entirely when used with an $L_1$ penalty, but not with an $L_2$ penalty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
