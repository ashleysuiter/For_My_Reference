{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n", " \n", "# Decision Trees\n", " \n", "_Author: Joseph Nelson (DC)_\n", "\n", "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Build a Decision Tree by Hand"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "\n", "import matplotlib.pyplot as plt\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["vehicle_prices_path = Path('..', 'assets', 'data', 'vehicles_train.csv')\n", "vehicle_prices = pd.read_csv(vehicle_prices_path)\n", "vehicle_prices.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Encode car as 0 and truck as 1.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# Make a scatter plot of vehicle price against each feature\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's split first on year < 2006.5"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"by-hand\"></a>\n", "**Exercise (10 mins., pair programming).** Continue to build a decision tree for this problem \"by hand.\"\n", "\n", "**Note:** You wouldn't typically develop a decision tree by hand. The purpose of this exercise is to help develop an understanding of what the computer does when it fits a decision tree."]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Split your DataFrame into two separate DataFrames according to your first splitting rule."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Within the first of those DataFrames, decide which feature you think is the most important predictor, and use it to choosen an additional splitting rule. (You can use the original feature again. You do not need to split the DataFrame again.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Within the second of those DataFrames, decide which feature you think is the most important predictor, and use it to choosen an additional splitting rule. (You can use the original feature again. You do not need to split the DataFrame again.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Draw a diagram of your tree, labeling the leaves with the mean price for the observations in that region. Make sure nothing is backwards: You follow the **left branch** if the rule is true and the **right branch** if the rule is false. You can either use some kind of drawing program or you computer or draw on paper and take a picture."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"computer-build\"></a>\n", "## How Does a Computer Build a Regression Tree?\n", "\n", "**Ideal approach:** Consider every possible partition of the feature space.\n", "\n", "**Problem:** Too many possibilities to consider.\n", "\n", "**\"Good enough\" approach:** Recursive binary splitting.\n", "\n", "1. Begin at the top of the tree.\n", "2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint so that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n", "3. Examine the two resulting regions. Once again, make a **single split** (in one of the regions) to minimize the MSE.\n", "4. Keep repeating Step 3 until a **stopping criterion** is met:\n", "    - Maximum tree depth (maximum number of splits required to arrive at a leaf).\n", "    - Minimum number of observations in a leaf.\n", "\n", "---\n", "\n", "This approach is a **greedy algorithm** because it makes *locally optimal* decisions -- it takes the best split at each step. Greedy algorithms typically are not optimal, but they are often good enough and relatively easy to compute.\n", "\n", "**Analogy:**\n", "- Always eating cookies to maximize your immediate happiness (greedy) might not lead to optimal overall happiness.\n", "- In our case, reorganizing parts of the tree already constructed based on future splits might result in a better model overall. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"cutpoint-demo\"></a>\n", "### Demo: Choosing the Ideal Cutpoint for a Given Feature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Before splitting anything, just predict the mean of the entire data set.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate RMSE for those predictions.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define a function that calculates the RMSE for a given split of miles.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate RMSE for tree that splits on miles < 50,000.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate RMSE for tree that splits on miles < 100,000.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check all possible mileage splits.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot mileage cutpoint (x-axis) versus RMSE (y-axis).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Recap:** Before every split, we repeat this process for every feature and choose the feature and cutpoint that produce the lowest MSE."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"sklearn-tree\"></a>\n", "## Building a Regression Tree in `scikit-learn`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define X and y.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate a DecisionTreeRegressor (with random_state=1).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"too-deep\"></a>\n", "## What Happens When We Grow a Tree Too Deep?\n", "\n", "- **On the left:** A regression tree for salary that is **grown deeper**.\n", "- **On the right:** A comparison of the **training, testing, and cross-validation errors** for trees with different numbers of leaves."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Salary tree grown deep](../assets/images/salary_tree_deep.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The **training error** continues to go down as the tree size increases (due to overfitting), but the lowest **cross-validation error** occurs for a tree with a depth of three.\n", "\n", "Note that if we make a **complete tree** (where every data point is boxed into its own region), then we will achieve perfect training accuracy. However, then outliers in the training data will greatly affect the model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"#tuning-tree\"></a>\n", "## Tuning a Regression Tree\n", "\n", "Let's try to reduce the RMSE by tuning the **max_depth** parameter:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Try different values one by one.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use a loop to try a range of values\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot max_depth (x-axis) versus RMSE (y-axis).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# max_depth=3 was best, so fit a tree using that parameter.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inspecting the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install tool to generate tree diagrams\n", "!conda install -y graphviz"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a tree diagram as a Graphviz file.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convert the graphviz file to PNG\n", "!dot -Tpng tree_vehicles.dot -o tree_vehicles.png"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Tree for vehicle data](../assets/images/tree_vehicles.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reading the internal nodes:\n", "\n", "- **samples:** Number of observations in that node before splitting.\n", "- **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node.\n", "- **rule:** Rule used to split that node (go left if true, go right if false).\n", "\n", "Reading the leaves:\n", "\n", "- **samples:** Number of observations in that node.\n", "- **value:** Mean response value in that node.\n", "- **mse:** MSE calculated by comparing the actual response values in that node against \"value.\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"testing-preds\"></a>\n", "## Making Predictions for the Testing Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Read the testing data.\n", "path = Path('..', 'assets', 'data', 'vehicles_test.csv')\n", "vehicle_prices_test = pd.read_csv(path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Change `vtype` to `is_truck` as in the training data\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (3 mins.).** Use the tree diagram above to get the model's prediction for the price of each item in the test set.\n", "\n", "For instance, for Row 0, the year is less than 2006.5, so you would go left at the first split. Then you would go left again because the mileage is less than 131,000. Then you would go right because the mileage is greater than 93,000. That split takes you to a leaf node with value = 4000, so the tree's prediction for Row 0 is \\$4000."]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Row 1"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Row 2"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- **BONUS**: Use the fitted model to check your answers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate RMSE\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (12 mins., in groups)**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It's up to you whether to work together or individually. The breakout rooms are just there so that you can help each other as needed."]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Create a regression tree for the \"hitters\" data using the features \"hits\" and \"years\" and the target \"salary\". Train it and get its MSE on the entire data set (with no train/test split). Discard any rows with missing data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["path = Path('..', 'assets', 'data', 'hitters.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- How might discarding rows with missing data generate bias? How big of an issue do you think that is in this case? How else could we handle the missing data?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Train your model and calculates its MSE in five-fold cross-validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- **BONUS:** Change your model to reduce cross-validation MSE.\n", "\n", "*Possible approaches:*\n", "\n", "- Change the `max_depth` of your tree.\n", "- Change [other hyperparameters](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html).\n", "- Change the set of features you use.\n", "- Do some \"feature engineering:\"\n", "    - Dummy-code categorical features to make them usable.\n", "    - Create new features by transforming or combining existing features (e.g. calculate batting averages or other rates). Note that a **monotonic transformation** that doesn't change the orders of the values (e.g. taking a non-negative variable to a power) doesn't make any difference to a decision tree because it can simply move its cutpoints accordingly.\n", "\n", "*Advice:*\n", "\n", "- Compare training-set performance to test-set performance to decide whether to make your model more or less complicated.\n", "- Plot your data to guide your feature selection and feature engineering efforts.\n", "- Look for natural feature transformations (e.g. calculating rates per at-bat) that might improve both the bias and the variance of your model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"part-two\"></a>\n", "# Part 2: Classification Trees\n", "\n", "**Example:** Predict whether Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008 (from the New York Times)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Obama-Clinton decision tree](../assets/images/obama_clinton_tree.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Exercise (5 mins., in groups):**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- What are the observational units?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What is the response variable?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What are the features?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What is the most predictive feature?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- Why does the tree split on high school graduation rate twice in a row?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What is the class prediction for the following county: 15 percent African American, 90 percent high school graduation rate, located in the South, high poverty, high population density?"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["- What would be the predicted probability for that same county, based on the observed frequency?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["$\\blacksquare$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"comparing-trees\"></a>\n", "## Comparing Regression Trees and Classification Trees\n", "\n", "|Regression Trees|Classification Trees|\n", "|---|---|\n", "|Predict a continuous response.|Predict a categorical response.|\n", "|Predict using mean response of each leaf.|Predict using most commonly occurring class of each leaf.|\n", "|Splits are chosen to minimize MSE.|Splits are chosen to minimize Gini index (discussed below).|"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"splitting-criteria\"></a>\n", "## Splitting Criteria for Classification Trees\n", "\n", "Common options for the splitting criteria:\n", "\n", "- **Classification error rate:** The fraction of training observations in a region that don't belong to the most common class.\n", "- **Gini index:** The measure of total variance across classes in a region."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Example: Classification Error Rate\n", "\n", "Pretend we are predicting whether or not someone will buy an iPhone or an Android:\n", "\n", "- At a particular node, there are **25 observations** (phone buyers) of whom **10 bought iPhones and 15 bought Androids**.\n", "- As the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n", "\n", "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n", "\n", "- **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android.\n", "- **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone.\n", "- Classification error rate after this split would be **5/25 = 20%**.\n", "\n", "Compare that with a split on age:\n", "\n", "- **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android.\n", "- **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android.\n", "- Classification error rate after this split would be **10/25 = 40%**.\n", "\n", "The decision tree algorithm will try **every possible split across all features** and choose the one that **reduces the error rate the most.**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Example: Gini Index\n", "\n", "Calculate the Gini index before making a split:\n", "\n", "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n", "\n", "- The **maximum value** of the Gini index is 0.5 and occurs when the classes are perfectly balanced in a node.\n", "- The **minimum value** of the Gini index is 0 and occurs when there is only one class represented in a node.\n", "- A node with a lower Gini index is said to be more \"pure.\"\n", "\n", "Evaluating the split on **gender** using the Gini index:\n", "\n", "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n", "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n", "$$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n", "\n", "Evaluating the split on **age** using the Gini index:\n", "\n", "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n", "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n", "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n", "\n", "Again, the decision tree algorithm will try **every possible split** and will choose the one that **reduces the Gini index (and thus increases the \"node purity\") the most**.\n", "\n", "You can think of this as each split increasing the accuracy of predictions. If there is some error at a node, then splitting at that node will result in two nodes with a higher average \"node purity\" than the original. So, we ensure continually better fits to the training data by continually splitting nodes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Classification Error Rate vs. Gini Index\n", "\n", "- Gini index is the default in sklearn.\n", "- One advantage Gini index is that it will sometimes make splits that do not improve accuracy but do give better predicted probabilities."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"sklearn-ctree\"></a>\n", "## Building a Classification Tree in `scikit-learn`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll build a classification tree using the Titanic survival data set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Read in the data.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titanic.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Encode female as 0 and male as 1.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["titanic.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fill in the missing values for age with the median age.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check the distribution of \"Embarked\"\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a DataFrame of dummy variables for Embarked.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Print the updated DataFrame.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- **Survived:** 0=died, 1=survived (response variable)\n", "- **Pclass:** 1=first class, 2=second class, 3=third class\n", "    - What will happen if the tree splits on this feature?\n", "- **Sex:** 0=female, 1=male\n", "- **Age:** Numeric value\n", "- **Embarked:** C or Q or S"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define X and y.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit a classification tree with max_depth=3 on all data.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a Graphviz file.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Export graphviz file to PNG\n", "!dot -Tpng ../assets/images/tree_titanic.dot -o ../assets/images/tree_titanic.png"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![Tree for Titanic data](../assets/images/tree_titanic.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice the split in the bottom right; the **same class** is predicted in both of its leaves. That split didn't affect the **classification error rate**, although it did increase the **node purity**. This is important because it increases the accuracy of our predicted probabilities.\n", "\n", "A useful side effect of measures such as the Gini index is that they can be used give some indication of feature importance:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Compute the feature importances (total reduction of Gini index brought by that feature)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<a id=\"part-three\"></a>\n", "# Summary: Comparing Decision Trees With Other Models\n", "\n", "**Advantages of decision trees:**\n", "\n", "- They can be used for regression or classification.\n", "- They can be displayed graphically.\n", "- They are highly interpretable.\n", "- They can be specified as a series of rules, and more closely approximate human decision-making than other models.\n", "- Prediction is fast.\n", "- Their features don't need scaling.\n", "- They automatically learn feature interactions.\n", "- Tends to ignore irrelevant features.\n", "- They are non-parametric -- as a result, they will outperform linear models if the relationship between features and response is highly non-linear."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Logistic regression on $X_1$ and $X_2$ yields a straight-line decision boundary, while a decision tree yields boxes. Which one is better depends on the relationship between the features and the response:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Left: Logistic regression decision boundary\n", "- Right: Decision tree decision boundary\n", "\n", "Green and yellow indicate the true classes.\n", "\n", "![Trees versus linear models](../assets/images/tree_vs_linear.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Disadvantages of decision trees:**\n", "\n", "- Their performance is (generally) not competitive with the best supervised learning methods.\n", "- They can easily overfit the training data (tuning is required).\n", "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree.\n", "- They don't tend to work well if the classes are highly unbalanced.\n", "- They don't tend to work well with very small data sets."]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}}, "nbformat": 4, "nbformat_minor": 2}